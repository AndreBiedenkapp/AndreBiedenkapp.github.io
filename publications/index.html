<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Andr√© Biedenkapp</title> <meta name="author" content="Andr√© Biedenkapp"/> <meta name="description" content="Publications are listed in reversed chronological order."/> <meta name="keywords" content="Dynamic Algorithm Configuration, Reinforcement Learning, Learning to Learn"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü§ñ</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://andrebiedenkapp.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Andr√©¬†</span>Biedenkapp</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link text-lowercase" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link text-lowercase" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link text-lowercase" href="/teaching/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link text-lowercase" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">Publications are listed in reversed chronological order.</p> </header> <article> <div class="publications"> <nav id="year-nav" class="navbar fixed-bottom container" style="margin-bottom: -50px; align-self: center;"> <p class="post-description" style="padding-bottom: 15px; align-self: center"> Jump to: <a href="#year-2026" class="btn btn-sm z-depth-0" style="padding: 0 0 0 0" role="button">2026</a> <a href="#year-2025" class="btn btn-sm z-depth-0" style="padding: 0 0 0 0" role="button">2025</a> <a href="#year-2024" class="btn btn-sm z-depth-0" style="padding: 0 0 0 0" role="button">2024</a> <a href="#year-2023" class="btn btn-sm z-depth-0" style="padding: 0 0 0 0" role="button">2023</a> <a href="#year-2022" class="btn btn-sm z-depth-0" style="padding: 0 0 0 0" role="button">2022</a> <a href="#year-2021" class="btn btn-sm z-depth-0" style="padding: 0 0 0 0" role="button">2021</a> <a href="#year-2020" class="btn btn-sm z-depth-0" style="padding: 0 0 0 0" role="button">2020</a> <a href="#year-2019" class="btn btn-sm z-depth-0" style="padding: 0 0 0 0" role="button">2019</a> <a href="#year-2018" class="btn btn-sm z-depth-0" style="padding: 0 0 0 0" role="button">2018</a> <a href="#year-2017" class="btn btn-sm z-depth-0" style="padding: 0 0 0 0" role="button">2017</a> </p> </nav> <h2 class="year" id="year-2026">2026</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="biedenka-aamas26a" class="col-sm-10"> <div class="title">Contextual Intelligence: The Next Leap for Reinforcement Learning</div> <div class="author"> <b>Andr√© Biedenkapp</b> </div> <div class="periodical"> <em>In Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems, AAMAS‚Äô26</em>, Blue Sky Ideas Track, 2026<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/26-AAMAS-Blue-Sky.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://openreview.net/forum?id=vtz8ZWRz67" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> </div> <div class="abstract hidden"> <p>Reinforcement learning (RL) has produced spectacular results in games, robotics, and continuous control. Yet, despite these successes, learned policies often fail to generalize beyond their training distribution, limiting real-world impact. Recent work on contextual RL (cRL) shows that exposing agents to environment characteristics ‚Äì contexts ‚Äì can improve zero-shot transfer. So far, the community has treated context as a monolithic, static observable, an approach that constrains the generalization capabilities of RL agents. To achieve contextual intelligence we first propose a novel taxonomy of contexts that separates allogenic (environment-imposed) from autogenic (agent-driven) factors. We identify three fundamental research directions that must be addressed to promote truly contextual intelligence: (1) Learning with heterogeneous contexts ‚Äì explicitly exploit the taxonomy levels so agents can reason about their influence on the world and vice versa; (2) Multi-time-scale modeling ‚Äì recognize that allogenic variables evolve slowly or remain static, whereas autogenic variables may change within an episode, potentially requiring different learning mechanisms; (3) Integration of abstract, high-level contexts ‚Äì incorporate roles, resource &amp; regulatory regimes, uncertainties, and other non-physical descriptors that crucially influence behavior. We envision context as a first-class modeling primitive, empowering agents to reason about who they are, what the world permits, and how both evolve over time. By doing so, we aim to catalyze a new generation of context-aware agents that can be deployed safely and efficiently in the real world.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">biedenka-aamas26a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Contextual Intelligence: The Next Leap for Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Biedenkapp, Andr√©}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems, {AAMAS}'26, Blue Sky Ideas Track}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{International Foundation for Autonomous Agents and Multiagent Systems / {ACM}}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year" id="year-2025">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="eimer-arxiv25a" class="col-sm-10"> <div class="title">Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network</div> <div class="author"> <a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a>,¬†Lennart Sch√§permeier,¬†<b>Andr√© Biedenkapp</b>,¬†Alexander Tornede,¬†Lars Kotthoff,¬†Pieter Leyman,¬†Matthias Feurer,¬†Katharina Eggensperger,¬†Kaitlin Maile,¬†Tanja Tornede,¬†Anna Kozak,¬†Ke Xue,¬†Marcel Wever,¬†Mitra Baratchi,¬†Damir Pulatov,¬†Heike Trautmann,¬†Haniye Kashgarani,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>arXiv:2512.16491</em>, 2025<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/arXiv:2512.16491" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing ex- periments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">eimer-arxiv25a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Eimer, Theresa and Sch√§permeier, Lennart and Biedenkapp, Andr√© and Tornede, Alexander and Kotthoff, Lars and Leyman, Pieter and Feurer, Matthias and Eggensperger, Katharina and Maile, Kaitlin and Tornede, Tanja and Kozak, Anna and Xue, Ke and Wever, Marcel and Baratchi, Mitra and Pulatov, Damir and Trautmann, Heike and Kashgarani, Haniye and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2512.16491}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="nguyen-arxiv25a" class="col-sm-10"> <div class="title">Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+(Œª,Œª))-GA</div> <div class="author"> T√†i Nguyen,¬†Phong Le,¬†<b>Andr√© Biedenkapp</b>,¬†<a href="http://www-ia.lip6.fr/~doerr/index.html" target="_blank" rel="noopener noreferrer">Carola Doerr</a>,¬†and¬†<a href="https://www.st-andrews.ac.uk/computer-science/people/nttd/" target="_blank" rel="noopener noreferrer">Nguyen Dang</a> </div> <div class="periodical"> <em>arXiv:2512.03805</em>, 2025<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/arXiv:2512.03805" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/taindp98/OneMax-DAC-Extension" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+( , ))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">nguyen-arxiv25a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1(Œª,Œª))-GA}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, T√†i and Le, Phong and Biedenkapp, Andr√© and Doerr, Carola and Dang, Nguyen}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2512.03805}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="prasanna-ewrl25a" class="col-sm-10"> <div class="title">One Does Not Simply Estimate State: Comparing Model-based and Model-free Reinforcement Learning on the Partially Observable MordorHike Benchmark</div> <div class="author"> Sai Prasanna,¬†<b>Andr√© Biedenkapp+</b>,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/rajan/" target="_blank" rel="noopener noreferrer">Raghu Rajan+</a> </div> <div class="periodical"> <em>In Eighteenth European Workshop on Reinforcement Learning</em>, 2025 <small><b>+Joint last authorship</b></small><br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://openreview.net/forum?id=lTyiOJwQqu" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> <a href="https://github.com/sai-prasanna/mordorhike" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Evaluating reinforcement learning agents on partially observable Markov decision processes remains lacking as common benchmarks often do not require complex state estimation under non-linear dynamics and noise. We introduce MordorHike, a benchmark suite for rigorous state estimation testing, revealing performance gaps which would not be possible on other benchmarks. We present an evaluation framework assessing task performance and state estimation quality via probing. Using this framework, we empirically compare model-based (Dreamer, R2I) and model-free (DRQN) agents for sophisticated state estimation. The analysis reveals that Dreamer excels in sample efficiency and achieves superior performance in the hardest setting while R2I underperforms, suggesting its linear recurrent architecture may be a bottleneck. Further analysis reveals links between state estimation quality and task performance. Finally, out-of-distribution analysis shows a generalization gap for all algorithms, although Dreamer maintains an edge in the most challenging setting. The results highlight the need for robust state estimation and the need for proper evaluation benchmarks while validating the usefulness of MordorHike for future POMDP research.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">prasanna-ewrl25a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{One Does Not Simply Estimate State: Comparing Model-based and Model-free Reinforcement Learning on the Partially Observable MordorHike Benchmark}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Prasanna, Sai and Biedenkapp, Andr√© and Rajan, Raghu}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Eighteenth European Workshop on Reinforcement Learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="mohan-ewrl25a" class="col-sm-10"> <div class="title">Mighty: A Comprehensive Tool for studying Generalization, Meta-RL and AutoRL</div> <div class="author"> Aditya Mohan*,¬†<a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer*</a>,¬†<a href="https://www.tnt.uni-hannover.de/en/staff/benjamin/" target="_blank" rel="noopener noreferrer">Carolin Benjamins</a>,¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a>,¬†and¬†<b>Andr√© Biedenkapp</b> </div> <div class="periodical"> <em>In Eighteenth European Workshop on Reinforcement Learning</em>, 2025 <small><b>*Joint first authorship</b></small><br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/25_ewrl_mighty.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://openreview.net/forum?id=QlDXH5NkUx" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> <a href="https://github.com/automl/Mighty" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Robust generalization, rapid adaptation, and automated tuning are essential for deploying reinforcement learning in real-world settings. However, research on these aspects remains scattered across non-standard codebases and custom orchestration scripts. We introduce Mighty, an open-source library that unifies Contextual Generalization, Meta-RL, and AutoRL under a single modular interface. Mighty cleanly separates a configurable Agent - specified by its learning algorithm, model architecture, replay buffer, exploration strategy, and hyperparameters - from a configurable environment modeled as a Contextual MDP in which transitions, rewards, and initial states are governed by context parameters. This design decouples inner‚Äêloop weight updates from outer‚Äêloop adaptations, enabling users to compose, within one framework, (i) contextual generalization and curriculum methods (e.g. Unsupervised Environment Design), (ii) bi‚Äêlevel meta‚Äêlearning (e.g. MAML, black‚Äêbox strategies), and (iii) automated hyperparameter and architecture search (e.g. Bayesian optimization, evolutionary strategies, population‚Äêbased training). We present Mighty‚Äôs design philosophy and core features and validate the ongoing base implementations on classic control and continuous control tasks. We hope that by providing a unified, modular interface, Mighty will simplify experimentation and inspire further advances in robust, adaptable reinforcement learning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mohan-ewrl25a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mighty: A Comprehensive Tool for studying Generalization, Meta-RL and AutoRL}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mohan, Aditya and Eimer, Theresa and Benjamins, Carolin and Lindauer, Marius and Biedenkapp, Andr√©}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Eighteenth European Workshop on Reinforcement Learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="nguyen-gecco25a" class="col-sm-10"> <div class="title">On the Importance of Reward Design in Reinforcement Learning-based Dynamic Algorithm Configuration: A Case Study on OneMax with (1+(Œª,Œª))-GA</div> <div class="author"> T√†i Nguyen,¬†Phong Le,¬†<b>Andr√© Biedenkapp</b>,¬†<a href="http://www-ia.lip6.fr/~doerr/index.html" target="_blank" rel="noopener noreferrer">Carola Doerr</a>,¬†and¬†<a href="https://www.st-andrews.ac.uk/computer-science/people/nttd/" target="_blank" rel="noopener noreferrer">Nguyen Dang</a> </div> <div class="periodical"> <em>In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO‚Äô25)</em>, 2025 <small>üèÖ<span style="color: #F29105 !important">Best paper award on the "Learning for Evolutionary Computation" track</span></small><br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/arXiv:2502.20265" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/taindp98/OneMax-DAC" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Dynamic Algorithm Configuration (DAC) has garnered significant attention in recent years, particularly in the prevalence of machine learning and deep learning algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges associated with algorithm configuration. However, making an RL agent work properly is a non-trivial task, especially in reward design, which necessitates a substantial amount of handcrafted knowledge based on domain expertise. In this work, we study the importance of reward design in the context of DAC via a case study on controlling the population size of the (1+(Œª,Œª))-GA optimizing OneMax. We observed that a poorly designed reward can hinder the RL agent‚Äôs ability to learn an optimal policy because of a lack of exploration, leading to both scalability and learning divergence issues. To address those challenges, we propose the application of a reward shaping mechanism to facilitate enhanced exploration of the environment by the RL agent. Our work not only demonstrates the ability of RL in dynamically configuring the (1+(Œª,Œª))-GA, but also confirms the advantages of reward shaping in the scalability of RL agents across various sizes of OneMax problems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nguyen-gecco25a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Importance of Reward Design in Reinforcement Learning-based Dynamic Algorithm Configuration: A Case Study on OneMax with (1(Œª,Œª))-GA}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, T√†i and Le, Phong and Biedenkapp, Andr√© and Doerr, Carola and Dang, Nguyen}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Genetic and Evolutionary Computation Conference ({GECCO}'25)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="shala-iclr25a" class="col-sm-10"> <div class="title">Efficient Cross-Episode Meta-RL</div> <div class="author"> <a href="https://relea.informatik.uni-freiburg.de/people/gresa-shala" target="_blank" rel="noopener noreferrer">Gresa Shala</a>,¬†<b>Andr√© Biedenkapp</b>,¬†Pierre Krack,¬†Florian Walter,¬†and¬†<a href="https://www.utn.de/en/departments/department-engineering/machine-learning-lab/" target="_blank" rel="noopener noreferrer">Josif Grabocka</a> </div> <div class="periodical"> <em>In Proceedings of the Thirteenth International Conference on Learning Representations</em>, 2025<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/2025_ECTMRL_ICLR.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://openreview.net/forum?id=UENQuayzr1" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> <a href="https://github.com/machinelearningnuremberg/ECET" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>We introduce Efficient Cross-Episodic Transformers (ECET), a new algorithm for online Meta-Reinforcement Learning that addresses the challenge of enabling reinforcement learning agents to perform effectively in previously unseen tasks. We demonstrate how past episodes serve as a rich source of in-context information, which our model effectively distills and applies to new contexts. Our learned algorithm is capable of outperforming the previous state-of-the-art and provides more efficient meta-training while significantly improving generalization capabilities. Experimental results, obtained across various simulated tasks of the MuJoCo, Meta-World and ManiSkill benchmarks, indicate a significant improvement in learning efficiency and adaptability compared to the state-of-the-art. Our approach enhances the agent‚Äôs ability to generalize from limited data and paves the way for more robust and versatile AI systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">shala-iclr25a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient Cross-Episode Meta-RL}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shala, Gresa and Biedenkapp, Andr√© and Krack, Pierre and Walter, Florian and Grabocka, Josif}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Thirteenth International Conference on Learning Representations}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="fernandes-arxiv25a" class="col-sm-10"> <div class="title">A Llama walks into the ‚ÄôBar‚Äô: Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam</div> <div class="author"> Rean Fernandes,¬†<b>Andr√© Biedenkapp</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/awad/" target="_blank" rel="noopener noreferrer">Noor Awad</a> </div> <div class="periodical"> <em>arXiv:2504.04945</em>, 2025<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/arXiv:2504.04945" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/ReanFernandes/bar-llama" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">fernandes-arxiv25a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Llama walks into the ‚ÄôBar‚Äô: Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fernandes, Rean and Biedenkapp, Andr√© and Hutter, Frank and Awad, Noor}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2504.04945}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="hog-tmlr25a" class="col-sm-10"> <div class="title">Meta-learning Population-based Methods for Reinforcement Learning</div> <div class="author"> Johannes Hog,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/rajan/" target="_blank" rel="noopener noreferrer">Raghu Rajan</a>,¬†<b>Andr√© Biedenkapp</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/awad/" target="_blank" rel="noopener noreferrer">Noor Awad</a>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†Vu Nguyen</div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, 2025<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/TMLR-MetaPB2-2025.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://openreview.net/forum?id=d9htascfP8" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> <a href="https://github.com/automl/MetaPB2" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Reinforcement learning (RL) algorithms are highly sensitive to their hyperparameter settings. Recently, numerous methods have been proposed to dynamically optimize these hyperparameters. One prominent approach is Population-Based Bandits (PB2), which uses time-varying Gaussian processes (GP) to dynamically optimize hyperparameters with a population of parallel agents. Despite its strong overall performance, PB2 experiences slow starts due to the GP initially lacking sufficient information. To mitigate this issue, we propose four different methods that utilize meta-data from various environments. These approaches are novel in that they adapt meta-learning methods to accommodate the time-varying setting. Among these approaches, MultiTaskPB2, which uses meta-learning for the surrogate model, stands out as the most promising approach. It outperforms PB2 and other baselines in both anytime and final performance across two RL environment families.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hog-tmlr25a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Meta-learning Population-based Methods for Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hog, Johannes and Rajan, Raghu and Biedenkapp, Andr√© and Awad, Noor and Hutter, Frank and Nguyen, Vu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2835-8856}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year" id="year-2024">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="ndir-ewrl24a" class="col-sm-10"> <div class="title">Inferring Behavior-Specific Context Improves Zero-Shot Generalization in Reinforcement Learning</div> <div class="author"> Tidiane Camaret Ndir,¬†<b>Andr√© Biedenkapp</b>,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/awad/" target="_blank" rel="noopener noreferrer">Noor Awad</a> </div> <div class="periodical"> <em>In Seventeenth European Workshop on Reinforcement Learning</em>, 2024<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://openreview.net/forum?id=51XSWH0mgN" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> <a href="http://arxiv.org/abs/arXiv:2404.09521" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/tidiane-camaret/contextual_rl_zero_shot" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>In this work, we address the challenge of zero-shot generalization (ZSG) in Reinforcement Learning (RL), where agents must adapt to entirely novel environments without additional training. We argue that understanding and utilizing contextual cues, such as the gravity level of the environment, is critical for robust generalization, and we propose to integrate the learning of context representations directly with policy learning. Our algorithm demonstrates improved generalization on various simulated domains, outperforming prior context-learning techniques in zero-shot settings. By jointly learning policy and context, our method acquires behavior-specific context representations, enabling adaptation to unseen environments and marks progress towards reinforcement learning systems that generalize across diverse real-world tasks. Our code and experiments are available at https://github.com/tidiane-camaret/contextual_rl_zero_shot</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ndir-ewrl24a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Inferring Behavior-Specific Context Improves Zero-Shot Generalization in Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ndir, Tidiane Camaret and Biedenkapp, Andr√© and Awad, Noor}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Seventeenth European Workshop on Reinforcement Learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="ferreira-owa24a" class="col-sm-10"> <div class="title">One-shot World Models Using a Transformer Trained on a Synthetic Prior</div> <div class="author"> Fabio Ferreira,¬†Moreno Schlageter,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/rajan/" target="_blank" rel="noopener noreferrer">Raghu Rajan</a>,¬†<b>Andr√© Biedenkapp</b>,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a> </div> <div class="periodical"> <em>In NeurIPS 2024 Workshop on Open-World Agents</em>, 2024<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/arXiv:2409.14084" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>A World Model is a compressed spatial and temporal representation of a real world environment that allows one to train an agent or execute planning methods. However, world models are typically trained on observations from the real world environment, and they usually do not enable learning policies for other real environments. We propose One-Shot World Model (OSWM), a transformer world model that is learned in an in-context learning fashion from purely synthetic data sampled from a prior distribution. Our prior is composed of multiple randomly initialized neural networks, where each network models the dynamics of each state and reward dimension of a desired target environment. We adopt the supervised learning procedure of Prior-Fitted Networks by masking next-state and reward at random context positions and query OSWM to make probabilistic predictions based on the remaining transition context. During inference time, OSWM is able to quickly adapt to the dynamics of a simple grid world, as well as the CartPole gym and a custom control environment by providing 1k transition steps as context and is then able to successfully train environment-solving agent policies. However, transferring to more complex environments remains a challenge, currently. Despite these limitations, we see this work as an important stepping-stone in the pursuit of learning world models purely from synthetic data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ferreira-owa24a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{One-shot World Models Using a Transformer Trained on a Synthetic Prior}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ferreira, Fabio and Schlageter, Moreno and Rajan, Raghu and Biedenkapp, Andr√© and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NeurIPS 2024 Workshop on Open-World Agents}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="bordne-automlws24a" class="col-sm-10"> <div class="title">CANDID DAC: Leveraging Coupled Action Dimensions with Importance Differences in DAC</div> <div class="author"> Philipp Bordne,¬†M. Asif Hasan,¬†Eddie Bergman,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/awad/" target="_blank" rel="noopener noreferrer">Noor Awad</a>,¬†and¬†<b>Andr√© Biedenkapp</b> </div> <div class="periodical"> <em>In Proceedings of the Third International Conference on Automated Machine Learning (AutoML 2024)</em>, Workshop Track, 2024<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://openreview.net/forum?id=ZCCZYfstkG" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> <a href="http://arxiv.org/abs/arXiv:2407.05789" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/PhilippBordne/candidDAC" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster/poster_candid_dac.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://www.youtube.com/watch?v=RtdkkK-JIM8&amp;feature=youtu.be" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>High-dimensional action spaces remain a challenge for dynamic algorithm configuration (DAC). Interdependencies and varying importance between action dimensions are further known key characteristics of DAC problems. We argue that these Coupled Action Dimensions with Importance Differences (CANDID) represent aspects of the DAC problem that are not yet fully explored. To address this gap, we introduce a new white-box benchmark within the DACBench suite that simulates the properties of CANDID. Further, we propose sequential policies as an effective strategy for managing these properties. Such policies factorize the action space and mitigate exponential growth by learning a policy per action dimension. At the same time, these policies accommodate the interdependence of action dimensions by fostering implicit coordination. We show this in an experimental study of value-based policies on our new benchmark. This study demonstrates that sequential policies significantly outperform independent learning of factorized policies in CANDID action spaces. In addition, they overcome the scalability limitations associated with learning a single policy across all action dimensions. The code used for our experiments is available under https://github.com/PhilippBordne/candidDAC.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bordne-automlws24a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CANDID DAC: Leveraging Coupled Action Dimensions with Importance Differences in DAC}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bordne, Philipp and Hasan, M. Asif and Bergman, Eddie and Awad, Noor and Biedenkapp, Andr√©}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Third International Conference on Automated Machine Learning (AutoML 2024), Workshop Track}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="prasanna-rlc24a" class="col-sm-10"> <div class="title">Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization</div> <div class="author"> Sai Prasanna*,¬†Karim Farid*,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/rajan/" target="_blank" rel="noopener noreferrer">Raghu Rajan</a>,¬†and¬†<b>Andr√© Biedenkapp</b> </div> <div class="periodical"> <em>Reinforcement Learning Journal</em>, 3, pp. 1317‚Äì1350, 2024 <small><b>*Joint first authorship</b></small><br> <small><span style="color: #82828299">Note: To also be presented at the Seventeenth European Workshop on Reinforcement Learning (EWRL 2024)</span></small> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/24_rlc_dreaming_of_many_worlds.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://openreview.net/forum?id=o8DrRuBsQb" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> <a href="http://arxiv.org/abs/arXiv:2403.10967" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/sai-prasanna/dreaming_of_many_worlds" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster/DOMW_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://rlj.cs.umass.edu/2024/papers/Paper167.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents. To address the broader challenge, we start with the simpler setting of contextual reinforcement learning (cRL), assuming observability of the context values that parameterize the variation in the system‚Äôs dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state. Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of the Dreamer (v3) (Hafner et al., 2023). This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics. Our experiments show that such systematic incorporation of the context improves the ZSG of the policies trained on the ‚Äúdreams‚Äù of the world model. We further find qualitatively that our approach allows Dreamer to disentangle the latent state from context, allowing it to extrapolate its dreams to the many worlds of unseen contexts. The code for all our experiments is available at https://github.com/sai-prasanna/dreaming_of_many_worlds.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">prasanna-rlc24a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Prasanna, Sai and Farid, Karim and Rajan, Raghu and Biedenkapp, Andr√©}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Reinforcement Learning Journal}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">issue</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1317--1350}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="shala-automlabcd24a" class="col-sm-10"> <div class="title">HPO-RL-Bench: A Zero-Cost Benchmark for HPO in Reinforcement Learning</div> <div class="author"> <a href="https://relea.informatik.uni-freiburg.de/people/gresa-shala" target="_blank" rel="noopener noreferrer">Gresa Shala</a>,¬†Sebastian Pineda Arango,¬†<b>Andr√© Biedenkapp</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.utn.de/en/departments/department-engineering/machine-learning-lab/" target="_blank" rel="noopener noreferrer">Josif Grabocka</a> </div> <div class="periodical"> <em>In Proceedings of the Third International Conference on Automated Machine Learning (AutoML‚Äô24)</em>, ABCD Track, 2024 <small>üèÖ<span style="color: #F29105 !important">Awarded runner up for the <a href="https://2024.automl.cc/?page_id=1406" target="_blank" rel="noopener noreferrer">best paper award</a></span></small><br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://openreview.net/forum?id=MlB61zPAeR" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> <a href="https://github.com/releaunifreiburg/HPO-RL-Bench" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/slides/HPO-RL-Bench.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://drive.google.com/file/d/1xAuplrgbY9qp5MU16aN2LExC8sNiAWMe/view?usp=sharing" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>Despite the undeniable importance of optimizing the hyperparameters of RL algorithms, existing state-of-the-art Hyperparameter Optimization (HPO) techniques are not frequently utilized by RL researchers. To catalyze HPO research in RL, we present a new large-scale benchmark that includes pre-computed reward curve evaluations of hyperparameter configurations for six established RL algorithms (PPO, DDPG, A2C, SAC, TD3, DQN) on 22 environments (Atari, Mujoco, Control), repeated for multiple seeds. We exhaustively computed the reward curves of all possible combinations of hyperparameters for the considered hyperparameter spaces for each RL algorithm in each environment. As a result, our benchmark permits zero-cost experiments for deploying and comparing new HPO methods. In addition, the benchmark offers a set of integrated HPO methods, enabling plug-and-play tuning of the hyperparameters of new RL algorithms, while pre-computed evaluations allow a zero-cost comparison of a new RL algorithm against the tuned RL baselines in our benchmark.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">shala-automlabcd24a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HPO-RL-Bench: A Zero-Cost Benchmark for HPO in Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shala, Gresa and Arango, Sebastian Pineda and Biedenkapp, Andr√© and Hutter, Frank and Grabocka, Josif}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Third International Conference on Automated Machine Learning (AutoML'24), ABCD Track}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="shala-arxiv24a" class="col-sm-10"> <div class="title">Hierarchical Transformers are Efficient Meta-Reinforcement Learners</div> <div class="author"> <a href="https://relea.informatik.uni-freiburg.de/people/gresa-shala" target="_blank" rel="noopener noreferrer">Gresa Shala</a>,¬†<b>Andr√© Biedenkapp</b>,¬†and¬†<a href="https://www.utn.de/en/departments/department-engineering/machine-learning-lab/" target="_blank" rel="noopener noreferrer">Josif Grabocka</a> </div> <div class="periodical"> <em>arXiv:2402.06402</em>, 2024<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/arXiv:2402.06402" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/releaunifreiburg/HT4MRL.git" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>We introduce Hierarchical Transformers for Meta-Reinforcement Learning (HTrMRL), a powerful online meta-reinforcement learning approach. HTrMRL aims to address the challenge of enabling reinforcement learning agents to perform effectively in previously unseen tasks. We demonstrate how past episodes serve as a rich source of information, which our model effectively distills and applies to new contexts. Our learned algorithm is capable of outperforming the previous state-of-the-art and provides more efficient meta-training while significantly improving generalization capabilities. Experimental results, obtained across various simulated tasks of the Meta-World Benchmark, indicate a significant improvement in learning efficiency and adaptability compared to the state-of-the-art on a variety of tasks. Our approach not only enhances the agent‚Äôs ability to generalize from limited data but also paves the way for more robust and versatile AI systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">shala-arxiv24a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hierarchical Transformers are Efficient Meta-Reinforcement Learners}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shala, Gresa and Biedenkapp, Andr√© and Grabocka, Josif}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2402.06402}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year" id="year-2023">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="rajan-jair23a" class="col-sm-10"> <div class="title">MDP Playground: An Analysis and Debug Testbed for Reinforcement Learning</div> <div class="author"> <a href="https://ml.informatik.uni-freiburg.de/profile/rajan/" target="_blank" rel="noopener noreferrer">Raghu Rajan</a>,¬†Jessica Lizeth Borja Diaz,¬†Suresh Guttikonda,¬†Fabio Ferreira,¬†<b>Andr√© Biedenkapp</b>,¬†Jan Ole Hartz,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a> </div> <div class="periodical"> <em>Journal of Artificial Intelligence Research (JAIR)</em>, 77, pp. 821‚Äì890, 2023<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://www.jair.org/index.php/jair/article/view/14314/26946" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="http://arxiv.org/abs/arXiv:1909.07750" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/automl/mdp-playground" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>We present MDP Playground, a testbed for Reinforcement Learning (RL) agents with dimensions of hardness that can be controlled independently to challenge agents in different ways and obtain varying degrees of hardness in toy and complex RL environments. We consider and allow control over a wide variety of dimensions, including delayed rewards, sequence lengths, reward density, stochasticity, image representations, irrelevant features, time unit, action range and more. We define a parameterised collection of fast-to-run toy environments in OpenAI Gym by varying these dimensions and propose to use these to understand agents better. We then show how to design experiments using MDP Playground to gain insights on the toy environments. We also provide wrappers that can inject many of these dimensions into any Gym environment. We experiment with these wrappers on Atari and Mujoco to allow for understanding the effects of these dimensions on environments that are more complex than the toy environments. We also compare the effect of the dimensions on the toy and complex environments. Finally, we show how to use MDP Playground to debug agents, to study the interaction of multiple dimensions and describe further use-cases.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rajan-jair23a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MDP Playground: An Analysis and Debug
  Testbed for Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rajan, Raghu and Diaz, Jessica Lizeth Borja and Guttikonda, Suresh and Ferreira, Fabio and Biedenkapp, Andr√© and von Hartz, Jan Ole and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Artificial Intelligence Research (JAIR)}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1613/jair.1.14314}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{77}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{821--890}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="benjamins-tmlr23a" class="col-sm-10"> <div class="title">Contextualize Me ‚Äì The Case for Context in Reinforcement Learning</div> <div class="author"> <a href="https://www.tnt.uni-hannover.de/en/staff/benjamin/" target="_blank" rel="noopener noreferrer">Carolin Benjamins</a>,¬†<a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a>,¬†Frederik Schubert,¬†Aditya Mohan,¬†Sebastian D√∂hler,¬†<b>Andr√© Biedenkapp</b>,¬†Bodo Rosenhahn,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, 2023<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/23-TMLR-CARL.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://openreview.net/forum?id=Y42xVBQusn" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> <a href="http://arxiv.org/abs/arXiv:2202.04500" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/automl/CARL" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster/EcoRL21_CARL.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://www.automl.org/contextual-rl/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="https://www.youtube.com/watch?v=V32MMhBPDBk" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>While Reinforcement Learning (RL) has made great strides towards solving increasingly complicated problems, many algorithms are still brittle to even slight environmental changes. Contextual Reinforcement Learning (cRL) provides a framework to model such changes in a principled manner, thereby enabling flexible, precise and interpretable task specification and generation. Our goal is to show how the framework of cRL contributes to improving zero-shot generalization in RL through meaningful benchmarks and structured reasoning about generalization tasks. We confirm the insight that optimal behavior in cRL requires context information, as in other related areas of partial observability. To empirically validate this in the cRL framework, we provide various context-extended versions of common RL environments. They are part of the first benchmark library, CARL, designed for generalization based on cRL extensions of popular benchmarks, which we propose as a testbed to further study general agents. We show that in the contextual setting, even simple RL environments become challenging - and that naive solutions are not enough to generalize across complex context spaces.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">benjamins-tmlr23a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Contextualize Me ‚Äì The Case for Context in Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Benjamins, Carolin and Eimer, Theresa and Schubert, Frederik and Mohan, Aditya and D√∂hler, Sebastian and Biedenkapp, Andr√© and Rosenhahn, Bodo and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2835-8856}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="shala-iclr23a" class="col-sm-10"> <div class="title">Gray-Box Gaussian Processes for Automated Reinforcement Learning</div> <div class="author"> <a href="https://relea.informatik.uni-freiburg.de/people/gresa-shala" target="_blank" rel="noopener noreferrer">Gresa Shala</a>,¬†<b>Andr√© Biedenkapp</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.utn.de/en/departments/department-engineering/machine-learning-lab/" target="_blank" rel="noopener noreferrer">Josif Grabocka</a> </div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR‚Äô23)</em>, 2023<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/23-ICLR-RCGP.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://openreview.net/forum?id=rmoMvptXK7M" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> <a href="https://github.com/releaunifreiburg/RCGP" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Despite having achieved spectacular milestones in an array of important real-world applications, most Reinforcement Learning (RL) methods are very brittle concerning their hyperparameters. Notwithstanding the crucial importance of setting the hyperparameters in training state-of-the-art agents, the task of hyperparameter optimization (HPO) in RL is understudied. In this paper, we propose a novel gray-box Bayesian Optimization technique for HPO in RL, that enriches Gaussian Processes with reward curve estimations based on generalized logistic functions. In a very large-scale experimental protocol, comprising 5 popular RL methods (DDPG, A2C, PPO, SAC, TD3), dozens of environments (Atari, Mujoco), and 7 HPO baselines, we demonstrate that our method significantly outperforms current HPO practices in RL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">shala-iclr23a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Gray-Box Gaussian Processes for Automated Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shala, Gresa and Biedenkapp, Andr√© and Hutter, Frank and Grabocka, Josif}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR'23)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year" id="year-2022">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="adriaens-jair22a" class="col-sm-10"> <div class="title">Automated Dynamic Algorithm Configuration</div> <div class="author"> Steven Adriaensen,¬†<b>Andr√© Biedenkapp</b>,¬†<a href="https://relea.informatik.uni-freiburg.de/people/gresa-shala" target="_blank" rel="noopener noreferrer">Gresa Shala</a>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/awad/" target="_blank" rel="noopener noreferrer">Noor Awad</a>,¬†<a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a>,¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a>,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a> </div> <div class="periodical"> <em>Journal of Artificial Intelligence Research (JAIR)</em>, 75, pp. 1633‚Äì1699, 2022<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://www.jair.org/index.php/jair/article/view/13922/26882" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="http://arxiv.org/abs/arXiv:2205.13881" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/automl/2022_JAIR_DAC_experiments" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.automl.org/automated-algorithm-design/dac/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>The performance of an algorithm often critically depends on its parameter configuration. While a variety of automated algorithm configuration methods have been proposed to relieve users from the tedious and error-prone task of manually tuning parameters, there is still a lot of untapped potential as the learned configuration is static, i.e., parameter settings remain fixed throughout the run. However, it has been shown that some algorithm parameters are best adjusted dynamically during execution, e.g., to adapt to the current part of the optimization landscape. Thus far, this is most commonly achieved through hand-crafted heuristics. A promising recent alternative is to automatically learn such dynamic parameter adaptation policies from data. In this article, we give the first comprehensive account of this new field of automated dynamic algorithm configuration (DAC), present a series of recent advances, and provide a solid foundation for future research in this field. Specifically, we (i) situate DAC in the broader historical context of AI research; (ii) formalize DAC as a computational problem; (iii) identify the methods used in prior-art to tackle this problem; (iv) conduct empirical case studies for using DAC in evolutionary optimization, AI planning, and machine learning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">adriaens-jair22a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automated Dynamic Algorithm Configuration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Adriaensen, Steven and Biedenkapp, Andr√© and Shala, Gresa and Awad, Noor and Eimer, Theresa and Lindauer, Marius and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Artificial Intelligence Research (JAIR)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{75}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1633--1699}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1613/jair.1.13922}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="biedenkapp22" class="col-sm-10"> <div class="title">Dynamic Algorithm Configuration by Reinforcement Learning</div> <div class="author"> <b>Andr√© Biedenkapp</b> </div> <div class="periodical"> PhD thesis, University of Freiburg, Department of Computer Science, Machine Learning Chair, 2022<br> <small><span style="color: #82828299">Note: Passed with Summa Cum Laude (best possible grade)</span></small> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://ml.informatik.uni-freiburg.de/wp-content/uploads/2022/11/2022_Dissertation_Andre_Biedenkapp.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://freidok.uni-freiburg.de/data/230869" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p></p> <p>The performance of algorithms, be it in the domain of machine learning, hard combinatorial problem solving or AI in general depends on their many parameters. Tuning an algorithm manually, however, is error-prone and very time-consuming. Many, if not most, algorithms are iterative in nature. Thus, they traverse a potentially diverse solution space, which might require different parameter settings at different stages to behave optimally. Further, algorithms are often used for solving a diverse set of problem instances, which by themselves might require different parameters. Taking all of this into account is infeasible for a human designer. Automated methods have therefore been proposed to mitigate human errors and minimize manual efforts. While such meta-algorithmic methods have shown large successes, there is still a lot of untapped potentials as prior approaches typically only consider configurations that do not change during an algorithm‚Äôs run or do not adapt to the problem instance.</p> <p>In this dissertation, we present the first framework that is capable of dynamically configuring algorithms, in other words, capable of adapting configurations to the problem instance at hand during an algorithm‚Äôs solving process. To this end, we model the dynamic algorithm configuration (DAC) problem as a contextual Markov decision process. This enables us to learn dynamic configuration policies in a data-driven way by means of reinforcement learning.</p> <p>We empirically demonstrate the effectiveness of our framework on a diverse set of problem settings consisting of artificial benchmarks, evolutionary algorithms, AI planning systems, as well as deep learning. We show that DAC outperforms previous meta-algorithmic approaches. Building on these successes, we formulate the first standardized interface for dynamic configuration and an extensive benchmark to facilitate reproducibility and lower the barrier of entry for new researchers into this novel research field. Lastly, our work on DAC feeds back into the reinforcement learning paradigm. Through the lens of DAC, we identify shortcomings in current state-of-the-art approaches and demonstrate how to solve these. In particular, intending to learn general policies for DAC, our work pushes the boundaries of generalization in reinforcement learning. We demonstrate how to efficiently incorporate domain knowledge when training general agents and propose to move from a reactive way of doing reinforcement learning to a proactive way by learning when to make new decisions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">biedenkapp22</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dynamic Algorithm Configuration by Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Biedenkapp, Andr√©}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Freiburg, Germany}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{University of Freiburg, Department of Computer Science, Machine Learning Chair}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="shala-metalearn22a" class="col-sm-10"> <div class="title">AutoRL-Bench 1.0</div> <div class="author"> <a href="https://relea.informatik.uni-freiburg.de/people/gresa-shala" target="_blank" rel="noopener noreferrer">Gresa Shala</a>,¬†Sebastian Pineda Arango,¬†<b>Andr√© Biedenkapp</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.utn.de/en/departments/department-engineering/machine-learning-lab/" target="_blank" rel="noopener noreferrer">Josif Grabocka</a> </div> <div class="periodical"> <em>In Workshop on Meta-Learning (MetaLearn@NeurIPS‚Äô22)</em>, 2022<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/22-metalearn-autorl_bench_1_0.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://openreview.net/forum?id=RyAl60VhTcG" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> </div> <div class="abstract hidden"> <p>It is well established that Reinforcement Learning (RL) is very brittle and sensitive to the choice of hyperparameters. This prevents RL methods from being usable out of the box. The field of automated RL (AutoRL) aims at automatically configuring the RL pipeline, to both make RL usable by a broader audience, as well as reveal its full potential. Still, there has been little progress towards this goal as new AutoRL methods often are evaluated with incompatible experimental protocols. Furthermore, the typically high cost of experimentation prevents a thorough and meaningful comparison of different AutoRL methods or established hyperparameter optimization (HPO) methods from the automated Machine Learning (AutoML) community. To alleviate these issues, we propose the first tabular AutoRL Benchmark for studying the hyperparameters of RL algorithms. We consider the hyperparameter search spaces of five well established RL methods (PPO, DDPG, A2C, SAC, TD3) across 22 environments for which we compute and provide the reward curves. This enables HPO methods to simply query our benchmark as a lookup table, instead of actually training agents. Thus, our benchmark offers a testbed for very fast, fair, and reproducible experimental protocols for comparing future black-box, gray-box, and online HPO methods for RL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">shala-metalearn22a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AutoRL-Bench 1.0}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shala, Gresa and Arango, Sebastian Pineda and Biedenkapp, Andr√© and Hutter, Frank and Grabocka, Josif}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Workshop on Meta-Learning (MetaLearn@NeurIPS'22)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="shala-metalearn22b" class="col-sm-10"> <div class="title">Gray-Box Gaussian Processes for Automated Reinforcement Learning</div> <div class="author"> <a href="https://relea.informatik.uni-freiburg.de/people/gresa-shala" target="_blank" rel="noopener noreferrer">Gresa Shala</a>,¬†<b>Andr√© Biedenkapp</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.utn.de/en/departments/department-engineering/machine-learning-lab/" target="_blank" rel="noopener noreferrer">Josif Grabocka</a> </div> <div class="periodical"> <em>In Workshop on Meta-Learning (MetaLearn@NeurIPS‚Äô22)</em>, 2022<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/22-metalearn-gray_box_gaussian_processes_fo.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://openreview.net/forum?id=oJp7uTL7ox-" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> </div> <div class="abstract hidden"> <p>Despite having achieved spectacular milestones in an array of important real-world applications, most Reinforcement Learning (RL) methods are very brittle concerning their hyperparameters. Notwithstanding the crucial importance of setting the hyperparameters in training state-of-the-art agents, the task of hyperparameter optimization (HPO) in RL is understudied. In this paper, we propose a novel gray-box Bayesian Optimization technique for HPO in RL, that enriches Gaussian Processes with reward curve estimations based on generalized logistic functions. We thus about the performance of learning algorithms, transferring information across configurations and about epochs of the learning algorithm. In a very large-scale experimental protocol, comprising 5 popular RL methods (DDPG, A2C, PPO, SAC, TD3), 22 environments (OpenAI Gym: Mujoco, Atari, Classic Control), and 7 HPO baselines, we demonstrate that our method significantly outperforms current HPO practices in RL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">shala-metalearn22b</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Gray-Box Gaussian Processes for Automated Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shala, Gresa and Biedenkapp, Andr√© and Hutter, Frank and Grabocka, Josif}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Workshop on Meta-Learning (MetaLearn@NeurIPS'22)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="sass-realml22a" class="col-sm-10"> <div class="title">DeepCAVE: An Interactive Analysis Tool for Automated Machine Learning</div> <div class="author"> Ren√© Sass,¬†Eddie Bergman,¬†<b>Andr√© Biedenkapp</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>In Workshop on Adaptive Experimental Design and Active Learning in the Real World (ReALML@ICML‚Äô22)</em>, 2022<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2206.03493.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="http://arxiv.org/abs/arXiv:2206.03493" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/automl/DeepCAVE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.automl.org/ixautoml/deepcave/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Automated Machine Learning (AutoML) is used more than ever before to support users in determining efficient hyperparameters, neural architectures, or even full machine learning pipelines. However, users tend to mistrust the optimization process and its results due to a lack of transparency, making manual tuning still widespread. We introduce DeepCAVE, an interactive framework to analyze and monitor state-of-the-art optimization procedures for AutoML easily and ad hoc. By aiming for full and accessible transparency, DeepCAVE builds a bridge between users and AutoML and contributes to establishing trust. Our framework‚Äôs modular and easy-to-extend nature provides users with automatically generated text, tables, and graphic visualizations. We show the value of DeepCAVE in an exemplary use-case of outlier detection, in which our framework makes it easy to identify problems, compare multiple runs and interpret optimization processes. The package is freely available on GitHub this https URL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sass-realml22a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DeepCAVE: An Interactive Analysis Tool for Automated Machine Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sass, Ren√© and Bergman, Eddie and Biedenkapp, Andr√© and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Workshop on Adaptive Experimental Design and Active Learning in the Real World (ReALML@ICML'22)}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2206.03493}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="biedenkapp-prl22a" class="col-sm-10"> <div class="title">Learning Domain-Independent Policies for Open List Selection</div> <div class="author"> <b>Andr√© Biedenkapp</b>,¬†<a href="https://speckdavid.github.io/" target="_blank" rel="noopener noreferrer">David Speck</a>,¬†Silvan Sievers,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a>,¬†and¬†Jendrik Seipp</div> <div class="periodical"> <em>In Workshop on Bridging the Gap Between AI Planning and Reinforcement Learning (PRL@ICAPS‚Äô22)</em>, 2022<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/22-PRL-DAC4AIPlanning.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/slides/2022_DIHeuSel@PRL-ICAPS.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://youtu.be/RgyYaJIr4p8" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>Since its proposal over a decade ago, LAMA has been considered one of the best-performing satisficing classical planners. Its key component is heuristic search with multiple open lists, each using a different heuristic function to order states. Even with a very simple, ad-hoc policy for open list selection, LAMA achieves state-of-the-art results. In this paper, we propose to use dynamic algorithm configuration to learn such policies in a principled and data-driven manner. On the learning side, we show how to train a reinforcement learning agent over several heterogeneous environments, aiming at zero-shot generalization to new related domains. On the planning side, our experimental results show that the trained policies often reach the performance of LAMA, and sometimes even perform better. Furthermore, our analysis of different policies shows that prioritizing states reached via preferred operators is crucial, explaining the strong performance of LAMA.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">biedenkapp-prl22a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Domain-Independent Policies for Open List Selection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Biedenkapp, Andr√© and Speck, David and Sievers, Silvan and Hutter, Frank and Lindauer, Marius and Seipp, Jendrik}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Workshop on Bridging the Gap Between AI Planning and Reinforcement Learning (PRL@ICAPS'22)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="biedenkapp-gecco22a" class="col-sm-10"> <div class="title">Theory-inspired Parameter Control Benchmarks for Dynamic Algorithm Configuration</div> <div class="author"> <b>Andr√© Biedenkapp*</b>,¬†<a href="https://www.st-andrews.ac.uk/computer-science/people/nttd/" target="_blank" rel="noopener noreferrer">Nguyen Dang*</a>,¬†Martin S. Krejca*,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="http://www-ia.lip6.fr/~doerr/index.html" target="_blank" rel="noopener noreferrer">Carola Doerr</a> </div> <div class="periodical"> <em>In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO‚Äô22)</em>, pp. 766‚Äì775, 2022 <small><b>*Joint first authorship</b></small> <small>üèÖ<span style="color: #F29105 !important">Won the best paper award on the <a href="https://gecco-2022.sigevo.org/Best-Paper-Awards#GECH_Track" target="_blank" rel="noopener noreferrer">GECH track.</a></span></small><br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/22-gecco.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="http://arxiv.org/abs/arXiv:2202.03259" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://andrebiedenkapp.github.io/blog/2022/gecco/" class="btn btn-sm z-depth-0" role="button">Blog</a> <a href="https://github.com/ndangtt/LeadingOnesDAC" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.automl.org/automated-algorithm-design/dac/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="https://youtu.be/xgljDu5qE-w" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>It has long been observed that the performance of evolutionary algorithms and other randomized search heuristics can benefit from a non-static choice of the parameters that steer their optimization behavior. Mechanisms that identify suitable configurations on the fly ("parameter control") or via a dedicated training process ("dynamic algorithm configuration") are therefore an important component of modern evolutionary computation frameworks. Several approaches to address the dynamic parameter setting problem exist, but we barely understand which ones to prefer for which applications. As in classical benchmarking, problem collections with a known ground truth can offer very meaningful insights in this context. Unfortunately, settings with well-understood control policies are very rare. One of the few exceptions for which we know which parameter settings minimize the expected runtime is the LeadingOnes problem. We extend this benchmark by analyzing optimal control policies that can select the parameters only from a given portfolio of possible values. This also allows us to compute optimal parameter portfolios of a given size. We demonstrate the usefulness of our benchmarks by analyzing the behavior of the DDQN reinforcement learning approach for dynamic algorithm configuration.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">biedenkapp-gecco22a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Theory-inspired Parameter Control Benchmarks for Dynamic Algorithm Configuration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Biedenkapp, Andr√© and Dang, Nguyen and Krejca, Martin S. and Hutter, Frank and Doerr, Carola}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{766--775}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Genetic and Evolutionary Computation Conference ({GECCO}'22)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="parker-holder-jair22a" class="col-sm-10"> <div class="title">Automated Reinforcement Learning (AutoRL): A Survey and Open Problems</div> <div class="author"> Jack Parker-Holder,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/rajan/" target="_blank" rel="noopener noreferrer">Raghu Rajan</a>,¬†Xingyou Song,¬†<b>Andr√© Biedenkapp</b>,¬†Yingjie Miao,¬†<a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a>,¬†Baohe Zhang,¬†Vu Nguyen,¬†Roberto Calandra,¬†Aleksandra Faust,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>Journal of Artificial Intelligence Research (JAIR)</em>, 74, pp. 517-568, 2022<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/22-JAIR-AutoRL.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="http://arxiv.org/abs/arXiv:2201.03916" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="/assets/pdf/poster/AutoRL_Survey_JAIR_2022_building_74.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://www.youtube.com/watch?v=6TcSGstpSUE&amp;list=PLp7L30nGpKM8LnCrbnT81KjBn5uxB6u8n&amp;index=3" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>The combination of Reinforcement Learning (RL) with deep learning has led to a series of impressive feats, with many believing (deep) RL provides a path towards generally capable agents. However, the success of RL agents is often highly sensitive to design choices in the training process, which may require tedious and error-prone manual tuning. This makes it challenging to use RL for new problems, while also limits its full potential. In many other areas of machine learning, AutoML has shown it is possible to automate such design choices and has also yielded promising initial results when applied to RL. However, Automated Reinforcement Learning (AutoRL) involves not only standard applications of AutoML but also includes additional challenges unique to RL, that naturally produce a different set of methods. As such, AutoRL has been emerging as an important area of research in RL, providing promise in a variety of applications from RNA design to playing games such as Go. Given the diversity of methods and environments considered in RL, much of the research has been conducted in distinct subfields, ranging from meta-learning to evolution. In this survey we seek to unify the field of AutoRL, we provide a common taxonomy, discuss each area in detail and pose open problems which would be of interest to researchers going forward.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">parker-holder-jair22a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automated Reinforcement Learning (AutoRL): A Survey and Open Problems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Parker-Holder, Jack and Rajan, Raghu and Song, Xingyou and Biedenkapp, Andr√© and Miao, Yingjie and Eimer, Theresa and Zhang, Baohe and Nguyen, Vu and Calandra, Roberto and Faust, Aleksandra and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Artificial Intelligence Research (JAIR)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{517-568}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{74}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1613/jair.1.13596}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="benjamins-arxiv22a" class="col-sm-10"> <div class="title">Contextualize Me ‚Äì The Case for Context in Reinforcement Learning</div> <div class="author"> <a href="https://www.tnt.uni-hannover.de/en/staff/benjamin/" target="_blank" rel="noopener noreferrer">Carolin Benjamins</a>,¬†<a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a>,¬†Frederik Schubert,¬†Aditya Mohan,¬†<b>Andr√© Biedenkapp</b>,¬†Bodo Rosenhan,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>arXiv:2202.04500</em>, 2022<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2202.04500.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="http://arxiv.org/abs/arXiv:2202.04500" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/automl/CARL/tree/icml_2022" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>While Reinforcement Learning (RL) has made great strides towards solving increasingly complicated problems, many algorithms are still brittle to even slight changes in environments. Contextual Reinforcement Learning (cRL) provides a theoretical framework to model such changes in a principled manner, thereby enabling flexible, precise and interpretable task specification and generation. Thus, cRL is an important formalization for studying generalization in RL. In this work, we reason about solving cRL in theory and practice. We show that theoretically optimal behavior in contextual Markov Decision Processes requires explicit context information. In addition, we empirically explore context-based task generation, utilizing context information in training and propose cGate, our state-modulating policy architecture. To this end, we introduce the first benchmark library designed for generalization based on cRL extensions of popular benchmarks, CARL. In short: Context matters! </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">benjamins-arxiv22a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Contextualize Me ‚Äì The Case for Context in Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Benjamins, Carolin and Eimer, Theresa and Schubert, Frederik and Mohan, Aditya and Biedenkapp, Andr√© and Rosenhan, Bodo and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2202.04500}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="lindauer-jmlr22a" class="col-sm-10"> <div class="title">SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization</div> <div class="author"> <a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a>,¬†Katharina Eggensperger,¬†Matthias Feurer,¬†<b>Andr√© Biedenkapp</b>,¬†Difan Deng,¬†<a href="https://www.tnt.uni-hannover.de/en/staff/benjamin/" target="_blank" rel="noopener noreferrer">Carolin Benjamins</a>,¬†Tim Ruhkopf,¬†Ren√© Sass,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a> </div> <div class="periodical"> <em>Journal of Machine Learning Research (JMLR) ‚Äì MLOSS</em>, 23(54), pp. 1-9, 2022<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/21-0888.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="http://arxiv.org/abs/2109.09831" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/automl/SMAC3" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.youtube.com/watch?v=-qdiIRXT-14&amp;list=PLp7L30nGpKM8LnCrbnT81KjBn5uxB6u8n&amp;index=7" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>Algorithm parameters, in particular hyperparameters of machine learning algorithms, can substantially impact their performance. To support users in determining well-performing hyperparameter configurations for their algorithms, datasets and applications at hand, SMAC3 offers a robust and flexible framework for Bayesian Optimization, which can improve performance within a few evaluations. It offers several facades and pre-sets for typical use cases, such as optimizing hyperparameters, solving low dimensional continuous (artificial) global optimization problems and configuring algorithms to perform well across multiple problem instances. The SMAC3 package is available under a permissive BSD-license at https://github.com/automl/SMAC3. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lindauer-jmlr22a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lindauer, Marius and Eggensperger, Katharina and Feurer, Matthias and Biedenkapp, Andr√© and Deng, Difan and Benjamins, Carolin and Ruhkopf, Tim and Sass, Ren√© and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Machine Learning Research (JMLR) -- MLOSS}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{54}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-9}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year" id="year-2021">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="benjamins-arxiv21a" class="col-sm-10"> <div class="title">CARL: A Benchmark for Contextual and Adaptive Reinforcement Learning</div> <div class="author"> <a href="https://www.tnt.uni-hannover.de/en/staff/benjamin/" target="_blank" rel="noopener noreferrer">Carolin Benjamins</a>,¬†<a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a>,¬†Frederik Schubert,¬†<b>Andr√© Biedenkapp</b>,¬†Bodo Rosenhan,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>In Workshop on Ecological Theory of Reinforcement Learning (EcoRL@NeurIPS‚Äô21)</em>, 2021<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2110.02102.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="http://arxiv.org/abs/arXiv:2110.02102" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/automl/CARL" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>While Reinforcement Learning has made great strides towards solving ever more complicated tasks, many algorithms are still brittle to even slight changes in their environment. This is a limiting factor for real-world applications of RL. Although the research community continuously aims at improving both robustness and generalization of RL algorithms, unfortunately it still lacks an open-source set of well-defined benchmark problems based on a consistent theoretical framework, which allows comparing different approaches in a fair, reliable and reproducibleway. To fill this gap, we propose CARL, a collection of well-known RL environments extended to contextual RL problems to study generalization. We show the urgent need of such benchmarks by demonstrating that even simple toy environments become challenging for commonly used approaches if different contextual instances of this task have to be considered. Furthermore, CARL allows us to provide first evidence that disentangling representation learning of the states from the policy learning with the context facilitates better generalization. By providing variations of diverse benchmarks from classic control, physical simulations, games and a real-world application of RNA design, CARL will allow the community to derive many more such insights on a solid empirical foundation. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">benjamins-arxiv21a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CARL: A Benchmark for Contextual and Adaptive Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Benjamins, Carolin and Eimer, Theresa and Schubert, Frederik and Biedenkapp, Andr√© and Rosenhan, Bodo and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Workshop on Ecological Theory of Reinforcement Learning (EcoRL@NeurIPS'21)}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2110.02102}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="eimer-ijcai21" class="col-sm-10"> <div class="title">DACBench: A Benchmark Library for Dynamic Algorithm Configuration</div> <div class="author"> <a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a>,¬†<b>Andr√© Biedenkapp</b>,¬†Maximilian Reimer,¬†Steven Adriaensen,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI‚Äô21)</em>, pp. 1668‚Äì1674, 2021<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/21-IJCAI-DACBench.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/paper/21-IJCAI-DACBench-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="http://arxiv.org/abs/2105.08541" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.automl.org/dacbench-benchmarking-dynamic-algorithm-configuration/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a> <a href="https://github.com/automl/DACBench" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.automl.org/automated-algorithm-design/dac/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="https://www.youtube.com/watch?v=-G-hLmBI4WM" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>Dynamic Algorithm Configuration (DAC) aims to dynamically control a target algorithm‚Äôs hyperparameters in order to improve its performance. Several theoretical and empirical results have demonstrated the benefits of dynamically controlling hyperparameters in domains like evolutionary computation, AI Planning or deep learning. Replicating these results, as well as studying new methods for DAC, however, is difficult since existing benchmarks are often specialized and incompatible with the same interfaces. To facilitate benchmarking and thus research on DAC, we propose DACBench, a benchmark library that seeks to collect and standardize existing DAC benchmarks from different AI domains, as well as provide a template for new ones. For the design of DACBench, we focused on important desiderata, such as (i) flexibility, (ii) reproducibility, (iii) extensibility and (iv) automatic documentation and visualization. To show the potential, broad applicability and challenges of DAC, we explore how a set of six initial benchmarks compare in several dimensions of difficulty. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">eimer-ijcai21</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DACBench: A Benchmark Library for Dynamic Algorithm Configuration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Eimer, Theresa and Biedenkapp, Andr√© and Reimer, Maximilian and Adriaensen, Steven and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Thirtieth International Joint Conference on
      Artificial Intelligence (IJCAI'21)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1668--1674}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ijcai.org}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="speck-et-al-icaps2021b" class="col-sm-10"> <div class="title">Learning Heuristic Selection with Dynamic Algorithm Configuration</div> <div class="author"> <a href="https://speckdavid.github.io/" target="_blank" rel="noopener noreferrer">David Speck*</a>,¬†<b>Andr√© Biedenkapp*</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†Robert Mattm√ºller,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>In Proceedings of the Thirty-First International Conference on Automated Planning and Scheduling (ICAPS 2021)</em>, pp. 597‚Äì605, 2021 <small><b>*Joint first authorship</b></small><br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/21-ICAPS-DAC-PLAN.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="http://arxiv.org/abs/2006.08246" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/speckdavid/rl-plan" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster/21-ICAPS-DAC-PLAN-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://www.automl.org/automated-algorithm-design/dac/dynamic-algorithm-configuration-for-ai-planning/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="https://icaps21.icaps-conference.org/exhibition/index.html?channel=152" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>A key challenge in satisficing planning is to use multiple heuristics within one heuristic search. An aggregation of multiple heuristic estimates, for example by taking the maximum, has the disadvantage that bad estimates of a single heuristic can negatively affect the whole search. Since the performance of a heuristic varies from instance to instance, approaches such as algorithm selection can be successfully applied. In addition, alternating between multiple heuristics during the search makes it possible to use all heuristics equally and improve performance. However, all these approaches ignore the internal search dynamics of a planning system, which can help to select the most useful heuristics for the current expansion step. We show that dynamic algorithm configuration can be used for dynamic heuristic selection which takes into account the internal search dynamics of a planning system. Furthermore, we prove that this approach generalizes over existing approaches and that it can exponentially improve the performance of the heuristic search. To learn dynamic heuristic selection, we propose an approach based on reinforcement learning and show empirically that domain-wise learned policies, which take the internal search dynamics of a planning system into account, can exceed existing approaches.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">speck-et-al-icaps2021b</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Speck, David and Biedenkapp, Andr{\'e} and Hutter, Frank and Mattm{\"u}ller, Robert and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Heuristic Selection with Dynamic Algorithm Configuration}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Goldman, Robert P. and Biundo, Susanne and Katz, Michael}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Thirty-First International Conference on Automated Planning and Scheduling (ICAPS 2021)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{AAAI Press}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{597--605}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="biedenkapp-icml21" class="col-sm-10"> <div class="title">TempoRL: Learning When to Act</div> <div class="author"> <b>Andr√© Biedenkapp</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/rajan/" target="_blank" rel="noopener noreferrer">Raghu Rajan</a>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>In Proceedings of the 38th International Conference on Machine Learning (ICML 2021)</em>, 139, pp. 914‚Äì924, 2021<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/biedenkapp21a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/paper/biedenkapp21a-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="http://arxiv.org/abs/2106.05262" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://andrebiedenkapp.github.io/blog/2022/temporl" class="btn btn-sm z-depth-0" role="button">Blog</a> <a href="https://github.com/automl/TempoRL" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster/tempoRL_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/slides/2021_TempoRL@ICML.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://slideslive.com/38959338/temporl-learning-when-to-act" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>Reinforcement learning is a powerful approach to learn behaviour through interactions with an environment. However, behaviours are usually learned in a purely reactive fashion, where an appropriate action is selected based on an observation. In this form, it is challenging to learn when it is necessary to execute new decisions. This makes learning inefficient, especially in environments that need various degrees of fine and coarse control. To address this, we propose a proactive setting in which the agent not only selects an action in a state but also for how long to commit to that action. Our TempoRL approach introduces skip connections between states and learns a skip-policy for repeating the same action along these skips. We demonstrate the effectiveness of TempoRL on a variety of traditional and deep RL environments, showing that our approach is capable of learning successful policies up to an order of magnitude faster than vanilla Q-learning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">biedenkapp-icml21</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TempoRL: Learning When to Act}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Biedenkapp, Andr√© and Rajan, Raghu and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{139}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{914--924}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 38th International Conference on Machine Learning (ICML 2021)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="eimer-icml21" class="col-sm-10"> <div class="title">Self-Paced Context Evaluations for Contextual Reinforcement Learning</div> <div class="author"> <a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a>,¬†<b>Andr√© Biedenkapp</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>In Proceedings of the 38th International Conference on Machine Learning (ICML 2021)</em>, 139, pp. 2948‚Äì2958, 2021<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/eimer21a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/paper/eimer21a-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="http://arxiv.org/abs/2106.05110" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.automl.org/self-paced-context-evaluation-for-contextual-reinforcement-learning/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a> <a href="https://github.com/automl/SPaCE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster/21-ICML-SPaCE-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://slideslive.com/38959253/selfpaced-context-evaluation-for-contextual-reinforcement-learning" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>Reinforcement learning (RL) has made a lot of advances for solving a single problem in a given environment; but learning policies that generalize to unseen variations of a problem remains challenging. To improve sample efficiency for learning on such instances of a problem domain, we present Self-Paced Context Evaluation (SPaCE). Based on self-paced learning, \spc automatically generates \task curricula online with little computational overhead. To this end, SPaCE leverages information contained in state values during training to accelerate and improve training performance as well as generalization capabilities to new instances from the same problem domain. Nevertheless, SPaCE is independent of the problem domain at hand and can be applied on top of any RL agent with state-value function approximation. We demonstrate SPaCE‚Äôs ability to speed up learning of different value-based RL agents on two environments, showing better generalization capabilities and up to 10x faster learning compared to naive approaches such as round robin or SPDRL, as the closest state-of-the-art approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">eimer-icml21</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Self-Paced Context Evaluations for Contextual Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Eimer, Theresa and Biedenkapp, Andr√© and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{139}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2948--2958}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 38th International Conference on Machine Learning (ICML 2021)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="viu-automlicml21a" class="col-sm-10"> <div class="title">Bag of Baselines for Multi-objective Joint Neural Architecture Search and Hyperparameter Optimization</div> <div class="author"> Sergio Izquierdo,¬†Julia Guerrero-Viu,¬†Sven Hauns,¬†Guilherme Miotto,¬†Simon Schrodi,¬†<b>Andr√© Biedenkapp</b>,¬†Thomas Elsken,¬†Difan Deng,¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a>,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a> </div> <div class="periodical"> <em>In Workshop on Automated Machine Learning (AutoML@ICML‚Äô21)</em>, 2021<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2105.01015.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="http://arxiv.org/abs/2105.01015" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/automl/multi-obj-baselines" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster/21-ARXIV-BBMoNASHPO-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://slideslive.com/38962449/bag-of-baselines-for-multiobjective-joint-neural-architecture-search-and-hyperparameter-optimization" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>Neural architecture search (NAS) and hyperparameter optimization (HPO) make deep learning accessible to non-experts by automatically finding the architecture of the deep neural network to use and tuning the hyperparameters of the used training pipeline. While both NAS and HPO have been studied extensively in recent years, NAS methods typically assume fixed hyperparameters and vice versa - there exists little work on joint NAS + HPO. Furthermore, NAS has recently often been framed as a multi-objective optimization problem, in order to take, e.g., resource requirements into account. In this paper, we propose a set of methods that extend current approaches to jointly optimize neural architectures and hyperparameters with respect to multiple objectives. We hope that these methods will serve as simple baselines for future research on multi-objective joint NAS + HPO. To facilitate this, all our code is available at this https URL. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">viu-automlicml21a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bag of Baselines for Multi-objective Joint Neural Architecture Search and Hyperparameter Optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Izquierdo, Sergio and Guerrero-Viu, Julia and Hauns, Sven and Miotto, Guilherme and Schrodi, Simon and Biedenkapp, Andr√© and Elsken, Thomas and Deng, Difan and Lindauer, Marius and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Workshop on Automated Machine Learning (AutoML@ICML'21)}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Workshop on Automated Machine Learning (AutoML@ICML'21)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="rajan-arxiv21" class="col-sm-10"> <div class="title">MDP Playground: A Design and Debug Testbed for Reinforcement Learning</div> <div class="author"> <a href="https://ml.informatik.uni-freiburg.de/profile/rajan/" target="_blank" rel="noopener noreferrer">Raghu Rajan</a>,¬†Jessica Lizeth Borja Diaz,¬†Suresh Guttikonda,¬†Fabio Ferreira,¬†<b>Andr√© Biedenkapp</b>,¬†Jan Ole Hartz,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a> </div> <div class="periodical"> <em>arXiv:1909.07750 [cs.LG]</em>, 2021<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/1909.07750.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="http://arxiv.org/abs/1909.07750" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/automl/mdp-playground" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>We present \emphMDP Playground, an efficient testbed for Reinforcement Learning (RL) agents with \textitorthogonal dimensions that can be controlled independently to challenge agents in different ways and obtain varying degrees of hardness in generated environments. We consider and allow control over a wide variety of dimensions, including \textitdelayed rewards, \textitrewardable sequences, \textitdensity of rewards, \textitstochasticity, \textitimage representations, \textitirrelevant features, \textittime unit, \textitaction range and more. We define a parameterised collection of fast-to-run toy environments in \textitOpenAI Gym by varying these dimensions and propose to use these for the initial design and development of agents. We also provide wrappers that inject these dimensions into complex environments from \textitAtari and \textitMujoco to allow for evaluating agent robustness. We further provide various example use-cases and instructions on how to use \textitMDP Playground to design and debug agents. We believe that \textitMDP Playground is a valuable testbed for researchers designing new, adaptive and intelligent RL agents and those wanting to unit test their agents.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rajan-arxiv21</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MDP Playground: A Design and Debug Testbed for Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rajan, Raghu and Diaz, Jessica Lizeth Borja and Guttikonda, Suresh and Ferreira, Fabio and Biedenkapp, Andr√© and von Hartz, Jan Ole and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:1909.07750 [cs.LG]}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="franke-iclr21a" class="col-sm-10"> <div class="title">Sample-Efficient Automated Deep Reinforcement Learning</div> <div class="author"> J√∂rg K H Franke,¬†Gregor K√∂hler,¬†<b>Andr√© Biedenkapp</b>,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a> </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR) 2021</em>, 2021<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2009.01555.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://openreview.net/forum?id=hSjxQ3B7GWq" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> <a href="http://arxiv.org/abs/2009.01555" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.automl.org/blog-autorl" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a> <a href="https://github.com/automl/SEARL" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://slideslive.com/38954141/sampleefficient-automated-deep-reinforcement-learning" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>Despite significant progress in challenging problems across various domains, applying state-of-the-art deep reinforcement learning (RL) algorithms remains challenging due to their sensitivity to the choice of hyperparameters. This sensitivity can partly be attributed to the non-stationarity of the RL problem, potentially requiring different hyperparameter settings at various stages of the learning process. Additionally, in the RL setting, hyperparameter optimization (HPO) requires a large number of environment interactions, hindering the transfer of the successes in RL to real-world applications. In this work, we tackle the issues of sample-efficient and dynamic HPO in RL. We propose a population-based automated RL (AutoRL) framework to meta-optimize arbitrary off-policy RL algorithms. In this framework, we optimize the hyperparameters and also the neural architecture while simultaneously training the agent. By sharing the collected experience across the population, we substantially increase the sample efficiency of the meta-optimization. We demonstrate the capabilities of our sample-efficient AutoRL approach in a case study with the popular TD3 algorithm in the MuJoCo benchmark suite, where we reduce the number of environment interactions needed for meta-optimization by up to an order of magnitude compared to population-based training.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">franke-iclr21a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sample-Efficient Automated Deep Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Franke, J√∂rg K H and K√∂hler, Gregor and Biedenkapp, Andr√© and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR) 2021}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR) 2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="zhang-aistats21" class="col-sm-10"> <div class="title">On the Importance of Hyperparameter Optimization for Model-based Reinforcement Learning</div> <div class="author"> Baohe Zhang,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/rajan/" target="_blank" rel="noopener noreferrer">Raghu Rajan</a>,¬†Luis Pineda,¬†Nathan Lambert,¬†<b>Andr√© Biedenkapp</b>,¬†Kurtland Chua,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†Roberto Calandra</div> <div class="periodical"> <em>In Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS)‚Äô21</em>, 130, pp. 4015‚Äì4023, 2021<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/zhang21n.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="http://proceedings.mlr.press/v130/zhang21n/zhang21n-supp.zip" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a> <a href="http://arxiv.org/abs/2102.13651" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://bair.berkeley.edu/blog/2021/04/19/mbrl/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a> <a href="https://github.com/automl/HPO_for_RL" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster/HPO_in_MBRL_AISTATS_2021_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://www.youtube.com/watch?v=lH0mgnjr1v4" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>Model-based Reinforcement Learning (MBRL) is a promising framework for learning control in a data-efficient manner. MBRL algorithms can be fairly complex due to the separate dynamics modeling and the subsequent planning algorithm, and as a result, they often possess tens of hyperpa- rameters and architectural choices. For this reason, MBRL typically requires significant human expertise before it can be applied to new problems and domains. To alleviate this problem, we propose to use automatic hyperparameter optimization (HPO). We demonstrate that this problem can be tackled effectively with automated HPO, which we demonstrate to yield significantly improved performance compared to human experts. In addition, we show that tuning of several MBRL hyperparameters dynamically, i.e. during the training itself, further improves the performance compared to using static hyperparameters which are kept fixed for the whole training. Finally, our experiments provide valuable insights into the effects of several hyperparameters, such as plan horizon or learning rate and their influence on the stability of training and resulting rewards.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang-aistats21</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Importance of Hyperparameter Optimization for Model-based Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Baohe and Rajan, Raghu and Pineda, Luis and Lambert, Nathan and Biedenkapp, Andr√© and Chua, Kurtland and Hutter, Frank and Calandra, Roberto}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{130}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4015--4023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{PMLR}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS)'21}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="mueller-metalearn21a" class="col-sm-10"> <div class="title">In-Loop Meta-Learning with Gradient-Alignment Reward</div> <div class="author"> Samuel M√ºller,¬†<b>Andr√© Biedenkapp</b>,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a> </div> <div class="periodical"> <em>In AAAI workshop on Meta-Learning Challenges</em>, 2021<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/21-OAML-MetaAAAI.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://colab.research.google.com/drive/1_RZd6O3rxZUHJlET9-3wYwV1WJJsLSJb?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>t the heart of the standard deep learning training loop is a greedy gradient step minimizing a given loss. We propose to add a second step to maximize training generalization. To do this, we optimize the loss of the next training step. While computing the gradient for this generally is very expensive and many interesting applications consider non-differentiable parameters (e.g. due to hard samples), we present a cheap-to-compute and memory-saving reward, the gradient-alignment reward (GAR), that can guide the optimization. We use this reward to optimize multiple distributions during model training. First, we present the application of GAR to choosing the data distribution as a mixture of multiple dataset splits in a small scale setting. Second, we show that it can successfully guide learning augmentation strategies competitive with state-of-the-art augmentation strategies on CIFAR-10 and CIFAR-100.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mueller-metalearn21a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{In-Loop Meta-Learning with Gradient-Alignment Reward}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{M√ºller, Samuel and Biedenkapp, Andr√© and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{AAAI workshop on Meta-Learning Challenges}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{AAAI workshop on Meta-Learning Challenges}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year" id="year-2020">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="awad-arxiv20a" class="col-sm-10"> <div class="title">Squirrel: A Switching Hyperparameter Optimizer Description of the entry by AutoML.org &amp; IOHprofiler to the NeurIPS 2020 BBO challenge</div> <div class="author"> <a href="https://ml.informatik.uni-freiburg.de/profile/awad/" target="_blank" rel="noopener noreferrer">Noor Awad</a>,¬†<a href="https://relea.informatik.uni-freiburg.de/people/gresa-shala" target="_blank" rel="noopener noreferrer">Gresa Shala</a>,¬†Difan Deng,¬†Neeratyoy Mallik,¬†Matthias Feurer,¬†Katharina Eggensperger,¬†<b>Andr√© Biedenkapp</b>,¬†Diederick Vermetten,¬†Hao Wang,¬†<a href="http://www-ia.lip6.fr/~doerr/index.html" target="_blank" rel="noopener noreferrer">Carola Doerr</a>,¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a>,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a> </div> <div class="periodical"> <em>arXiv:2012.08180 [cs.LG]</em>, 2020 <small>üèÖ<span style="color: #F29105 !important">Winner of the NeurIPS 2020 BBO challenge on the <a href="https://bbochallenge.com/altleaderboard/" target="_blank" rel="noopener noreferrer">meta-learning friendly track</a></span></small><br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2012.08180.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="http://arxiv.org/abs/2012.08180" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/automl/Squirrel-Optimizer-BBO-NeurIPS20-automlorg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://bbochallenge.com/virtualroom/" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>In this short note, we describe our submission to the NeurIPS 2020 BBO challenge. Motivated by the fact that different optimizers work well on different problems, our approach switches between different optimizers. Since the team names on the competition‚Äôs leaderboard were randomly generated ‚Äùalliteration nicknames‚Äù, consisting of an adjective and an animal with the same initial letter, we called our approach the Switching Squirrel, or here, short, Squirrel.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">awad-arxiv20a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Squirrel: A Switching Hyperparameter Optimizer Description of the entry by AutoML.org  IOHprofiler to the NeurIPS 2020 BBO challenge}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Awad, Noor and Shala, Gresa and Deng, Difan and Mallik, Neeratyoy and Feurer, Matthias and Eggensperger, Katharina and Biedenkapp, Andr√© and Vermetten, Diederick and Wang, Hao and Doerr, Carola and Lindauer, Marius and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2012.08180 [cs.LG]}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="speck-et-al-icaps2020wsprl" class="col-sm-10"> <div class="title">Learning Heuristic Selection with Dynamic Algorithm Configuration</div> <div class="author"> <a href="https://speckdavid.github.io/" target="_blank" rel="noopener noreferrer">David Speck*</a>,¬†<b>Andr√© Biedenkapp*</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†Robert Mattm√ºller,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>In ICAPS 2020 Workshop on Bridging the Gap Between AI Planning and Reinforcement Learning (PRL)</em>, pp. 61‚Äì69, 2020 <small><b>*Joint first authorship</b></small><br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2006.08246v2.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="http://arxiv.org/abs/2006.08246v2" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/speckdavid/rl-plan" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.automl.org/automated-algorithm-design/dac/dynamic-algorithm-configuration-for-ai-planning/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="https://icaps21.icaps-conference.org/exhibition/index.html?channel=152" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>A key challenge in satisficing planning is to use multiple heuristics within one heuristic search. An aggregation of multiple heuristic estimates, for example by taking the maximum, has the disadvantage that bad estimates of a single heuristic can negatively affect the whole search. Since the performance of a heuristic varies from instance to instance, approaches such as algorithm selection can be successfully applied. In addition, alternating between multiple heuristics during the search makes it possible to use all heuristics equally and improve performance. However, all these approaches ignore the internal search dynamics of a planning system, which can help to select the most helpful heuristics for the current expansion step. We show that dynamic algorithm configuration can be used for dynamic heuristic selection which takes into account the internal search dynamics of a planning system. Furthermore, we prove that this approach generalizes over existing approaches and that it can exponentially improve the performance of the heuristic search. To learn dynamic heuristic selection, we propose an approach based on reinforcement learning and show empirically that domain-wise learned policies, which take the internal search dynamics of a planning system into account, can exceed existing approaches in terms of coverage.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">speck-et-al-icaps2020wsprl</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Heuristic Selection with Dynamic Algorithm Configuration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Speck, David and Biedenkapp, Andr{\'e} and Hutter, Frank and Mattm{\"u}ller, Robert and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICAPS 2020 Workshop on Bridging the Gap Between AI Planning and Reinforcement Learning (PRL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{61--69}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="shala-ppsn20" class="col-sm-10"> <div class="title">Learning Step-Size Adaptation in CMA-ES</div> <div class="author"> <a href="https://relea.informatik.uni-freiburg.de/people/gresa-shala" target="_blank" rel="noopener noreferrer">Gresa Shala*</a>,¬†<b>Andr√© Biedenkapp*</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/awad/" target="_blank" rel="noopener noreferrer">Noor Awad</a>,¬†Steven Adriaensen,¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a>,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a> </div> <div class="periodical"> <em>In Proceedings of the Sixteenth International Conference on Parallel Problem Solving from Nature (PPSN‚Äô20)</em>, 12269, pp. 691‚Äì706, 2020 <small><b>*Joint first authorship</b></small><br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/20-PPSN-LTO-CMA.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.automl.org/learning-step-size-adaptation-in-cma-es/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a> <a href="https://github.com/automl/LTO-CMA" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster/20-PPSN-LTO-CMA-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://www.automl.org/automated-algorithm-design/dac/dac-es/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="https://drive.google.com/file/d/1bhrbdUu-U76iJJtKYhPV3sdo4nI6N6XO/view" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>An algorithm‚Äôs parameter setting often affects its ability to solve a given problem, e.g., population-size, mutation-rate or crossover- rate of an evolutionary algorithm. Furthermore, some parameters have to be adjusted dynamically, such as lowering the mutation-strength over time. While hand-crafted heuristics offer a way to fine-tune and dynamically configure these parameters, their design is tedious, time-consuming and typically involves analyzing the algorithm‚Äôs behavior on simple problems that may not be representative for those that arise in practice. In this paper, we show that formulating dynamic algorithm configuration as a reinforcement learning problem allows us to automatically learn policies that can dynamically configure the mutation step-size parameter of Covariance Matrix Adaptation Evolution Strategy (CMA-ES). We evaluate our approach on a wide range of black-box optimization problems, and show that (i) learning step-size policies has the potential to improve the performance of CMA-ES; (ii) learned step-size policies can outperform the default Cumulative Step-Size Adaptation of CMA-ES; and transferring the policies to (iii) different function classes and to (iv) higher dimensions is also possible.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">shala-ppsn20</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Step-Size Adaptation in CMA-ES}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shala, Gresa and Biedenkapp, Andr√© and Awad, Noor and Adriaensen, Steven and Lindauer, Marius and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Lecture Notes in Computer Science}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{12269}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{691--706}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Sixteenth International Conference on Parallel Problem Solving from Nature (PPSN'20)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="biedenkapp-bigicml20" class="col-sm-10"> <div class="title">Towards TempoRL: Learning When to Act</div> <div class="author"> <b>Andr√© Biedenkapp</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/rajan/" target="_blank" rel="noopener noreferrer">Raghu Rajan</a>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>In Workshop on Inductive Biases, Invariances and Generalization in RL (BIG@ICML‚Äô20)</em>, 2020<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/20-BIG-TempoRL.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/automl/TabularTempoRL" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/slides/20-BIG-TempoRL-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://slideslive.com/38931351/towards-temporl-learning-when-to-act?ref=speaker-36467" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>Reinforcement Learning is a powerful approach to learning behaviour through interactions with an environment. However, behaviours are learned in a purely reactive fashion, where an appropriate action is selected based on an observation. In this form, it is challenging to learn when it is necessary to make new decisions. This makes learning inefficient especially in environments with with very fine-grained time steps. Instead we propose a more proactive setting in which not only an action is chosen in a state but also for how long to commit to that action. We demonstrate the effectiveness of our proposed approach on a set of small grid worlds, showing that our approach is capable of learning successful policies much faster than vanilla Q-learning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">biedenkapp-bigicml20</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards TempoRL: Learning When to Act}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Biedenkapp, Andr√© and Rajan, Raghu and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Workshop on Inductive Biases, Invariances and Generalization in RL (BIG@ICML'20)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="eimer-bigicml20" class="col-sm-10"> <div class="title">Towards Self-Paced Context Evaluations for Contextual Reinforcement Learning</div> <div class="author"> <a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a>,¬†<b>Andr√© Biedenkapp</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>In Workshop on Inductive Biases, Invariances and Generalization in RL (BIG@ICML‚Äô20)</em>, 2020<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/20-BIG-SPaCE.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/automl/SPaCE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://slideslive.com/38931344/towards-selfpaced-context-evaluation-for-contextual-reinforcement-learning?ref=speaker-36467" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>Reinforcement Learning has performed very well on games and lab-based tasks. However, learning policies across a distribution of instances of the same task still remains challenging. Recent approaches assume either little variation between instances or an unlimited amount of training examples from a given distribution. Both properties are not always feasible in real-world applications. Thus, we need methods that enable agents to generalize from a limited set of example instances or experiences. We present an approach, based on self-paced learning, that allows to exploit the information contained in state values during training to accelerate and improve training performance as well as generalization capabilities, independent of the problem domain at hand. The proposed Self-Paced Context Evaluation (SPaCE) provides a way to automatically generate instance curricula online with little computational overhead.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">eimer-bigicml20</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Self-Paced Context Evaluations for Contextual Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Eimer, Theresa and Biedenkapp, Andr√© and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Workshop on Inductive Biases, Invariances and Generalization in RL (BIG@ICML'20)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="biedenkapp-ecai20" class="col-sm-10"> <div class="title">Dynamic Algorithm Configuration: Foundation of a New Meta-Algorithmic Framework</div> <div class="author"> <b>Andr√© Biedenkapp</b>,¬†Furkan H Bozkurt,¬†<a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>In Proceedings of the Twenty-fourth European Conference on Artificial Intelligence (ECAI‚Äô20)</em>, pp. 427‚Äì434, 2020<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/20-ecai-dac.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/automl/DAC/blob/master/Appendix.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a> <a href="https://www.automl.org/dynamic-algorithm-configuration/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a> <a href="https://github.com/automl/DAC" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/slides/2020_DAC@ECAI.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://www.youtube.com/watch?v=wxPYtSGT05s&amp;feature=youtu.be" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>The performance of many algorithms in the fields of hard combinatorial problem solving, machine learning or AI in general depends on parameter tuning. Automated methods have been proposed to alleviate users from the tedious and error-prone task of manually searching for performance-optimized configurations across a set of problem instances. However, there is still a lot of untapped potential through adjusting an algorithm‚Äôs parameters online since different parameter values can be optimal at different stages of the algorithm. Prior work showed that reinforcement learning is an effective approach to learn policies for online adjustments of algorithm parameters in a data-driven way. We extend that approach by formulating the resulting dynamic algorithm configuration as a contextual MDP, such that RL not only learns a policy for a single instance, but across a set of instances. To lay the foundation for studying dynamic algorithm configuration with RL in a controlled setting, we propose white-box benchmarks covering major aspects that make dynamic algorithm configuration a hard problem in practice and study the per- formance of various types of configuration strategies for them. On these white-box benchmarks, we show that (i) RL is a robust candidate for learning configuration policies, outperforming standard pa- rameter optimization approaches, such as classical algorithm configuration; (ii) based on function approximation, RL agents can learn to generalize to new types of instances; and (iii) self-paced learning can substantially improve the performance by selecting a useful sequence of training instances automatically.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">biedenkapp-ecai20</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dynamic Algorithm Configuration: Foundation of a New Meta-Algorithmic Framework}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Biedenkapp, Andr√© and Bozkurt, Furkan H and Eimer, Theresa and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{427--434}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Twenty-fourth European Conference on Artificial Intelligence (ECAI'20)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year" id="year-2019">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="biedenkapp-dso19" class="col-sm-10"> <div class="title">Towards White-box Benchmarks for Algorithm Control</div> <div class="author"> <b>Andr√© Biedenkapp</b>,¬†Furkan H. Bozkurt,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>In IJCAI 2019 DSO Workshop</em>, 2019<br> <small><span style="color: #82828299">Note: In this early work on DAC we refered to "dynamic algorithm configuraiton" as "algorithm control"</span></small> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/1906.07644.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="http://arxiv.org/abs/1906.07644" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="/assets/pdf/poster/Whitebox@COSEAL_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/slides/2019_DSO_WBforAlgorithmControl.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://www.automl.org/automated-algorithm-design/dac/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>The performance of many algorithms in the fields of hard combinatorial problem solving, machine learning or AI in general depends on tuned hyperparameter configurations. Automated methods have been proposed to alleviate users from the tedious and error-prone task of manually searching for performance-optimized configurations across a set of problem instances. However there is still a lot of untapped potential through adjusting an algorithm‚Äôs hyperparameters online since different hyperparameters are potentially optimal at different stages of the algorithm. We formulate the problem of adjusting an algorithm‚Äôs hyperparameters for a given instance on the fly as a contextual MDP, making reinforcement learning (RL) the prime candidate to solve the resulting algorithm control problem in a data-driven way. Furthermore, inspired by applications of algorithm configuration, we introduce new white-box benchmarks suitable to study algorithm control. We show that on short sequences, algorithm configuration is a valid choice, but that with increasing sequence length a black-box view on the problem quickly becomes infeasible and RL performs better.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">biedenkapp-dso19</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards White-box Benchmarks for Algorithm Control}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Biedenkapp, Andr√© and Bozkurt, Furkan H. and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IJCAI 2019 DSO Workshop}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="lindauer-arxiv19" class="col-sm-10"> <div class="title">BOAH: A Tool Suite for Multi-Fidelity Bayesian Optimization &amp; Analysis of Hyperparameters</div> <div class="author"> <a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a>,¬†Katharina Eggensperger,¬†Matthias Feurer,¬†<b>Andr√© Biedenkapp</b>,¬†Joshua Marben,¬†Philipp M√ºller,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a> </div> <div class="periodical"> <em>arXiv:1908.06756 [cs.LG]</em>, 2019<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/1908.06756.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="http://arxiv.org/abs/arXiv:1908.06756" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/automl/BOAH" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.ml4aad.org/ixautoml/boah" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Hyperparameter optimization and neural architecture search can become prohibitively expensive for regular black-box Bayesian optimization because the training and evaluation of a single model can easily take several hours. To overcome this, we introduce a comprehensive tool suite for effective multi-fidelity Bayesian optimization and the analysis of its runs. The suite, written in Python, provides a simple way to specify complex design spaces, a robust and efficient combination of Bayesian optimization and HyperBand, and a comprehensive analysis of the optimization process and its outcomes.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lindauer-arxiv19</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BOAH: A Tool Suite for Multi-Fidelity Bayesian Optimization  Analysis of Hyperparameters}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lindauer, Marius and Eggensperger, Katharina and Feurer, Matthias and Biedenkapp, Andr√© and Marben, Joshua and M√ºller, Philipp and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:1908.06756 [cs.LG]}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="lindauer-dso19" class="col-sm-10"> <div class="title">Towards Assessing the Impact of Bayesian Optimization‚Äôs Own Hyperparameters</div> <div class="author"> <a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a>,¬†Matthias Feurer,¬†Katharina Eggensperger,¬†<b>Andr√© Biedenkapp</b>,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a> </div> <div class="periodical"> <em>In IJCAI 2019 DSO Workshop</em>, 2019<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/1908.06674.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="http://arxiv.org/abs/1908.06674" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="/assets/pdf/slides/19-DSO_BOBO-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Bayesian Optimization (BO) is a common approach for hyperparameter optimization (HPO) in automated machine learning. Although it is well-accepted that HPO is crucial to obtain well-performing machine learning models, tuning BO‚Äôs own hyperparameters is often neglected. In this paper, we empirically study the impact of optimizing BO‚Äôs own hyperparameters and the transferability of the found settings using a wide range of benchmarks, including artificial functions, HPO and HPO combined with neural architecture search. In particular, we show (i) that tuning can improve the any-time performance of different BO approaches, that optimized BO settings also perform well (ii) on similar problems and (iii) partially even on problems from other problem families, and (iv) which BO hyperparameters are most important.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lindauer-dso19</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Assessing the Impact of Bayesian Optimization's Own Hyperparameters}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lindauer, Marius and Feurer, Matthias and Eggensperger, Katharina and Biedenkapp, Andr√© and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IJCAI 2019 DSO Workshop}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year" id="year-2018">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="biedenkapp-lion18a" class="col-sm-10"> <div class="title">CAVE: Configuration Assessment, Visualization and Evaluation</div> <div class="author"> <b>Andr√© Biedenkapp</b>,¬†Joshua Marben,¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a>,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a> </div> <div class="periodical"> <em>In Proceedings of the International Conference on Learning and Intelligent Optimization (LION‚Äô18)</em>, 11353, pp. 115‚Äì130, 2018<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/18-LION12-CAVE.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/automl/CAVE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/slides/18-LION12-CAVE-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://www.automl.org/ixautoml/cave/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="https://drive.google.com/file/d/1lNu6sZGB3lcr6fYI1tzLOJzILISO9WE1/view" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>To achieve peak performance of an algorithm (in particular for problems in AI), algorithm configuration is often necessary to determine a well-performing parameter configuration. So far, most studies in algorithm configuration focused on proposing better algorithm configuration procedures or on improving a particular algorithm‚Äôs performance. In contrast, we use all the collected empirical performance data gathered during algorithm configuration runs to generate extensive insights into an algorithm, given problem instances and the used configurator. To this end, we provide a tool, called CAVE , that automatically generates comprehensive reports and insightful figures from all available empirical data. CAVE aims to help algorithm and configurator developers to better understand their experimental setup in an automated fashion. We showcase its use by thoroughly analyzing the well studied SAT solver spear on a benchmark of software verification instances and by empirically verifying two long-standing assumptions in algorithm configuration and parameter importance: (i) Parameter importance changes depending on the instance set at hand and (ii) Local and global parameter importance analysis do not necessarily agree with each other.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">biedenkapp-lion18a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CAVE: Configuration Assessment, Visualization and Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Biedenkapp, Andr√© and Marben, Joshua and Lindauer, Marius and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{11353}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{115--130}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Learning and Intelligent Optimization (LION'18)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year" id="year-2017">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="biedenkapp-aaai17a" class="col-sm-10"> <div class="title">Efficient Parameter Importance Analysis via Ablation with Surrogates</div> <div class="author"> <b>Andr√© Biedenkapp</b>,¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a>,¬†Katharina Eggensperger,¬†Chris Fawcett,¬†Holger H Hoos,¬†and¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a> </div> <div class="periodical"> <em>In Proceedings of the Thirty-First Conference on Artificial Intelligence (AAAI‚Äô17)</em>, pp. 773‚Äì779, 2017<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/surrogate_ablation_aaai17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://bitbucket.org/biedenka/ablation/src/master/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster/surrogate_ablation_aaai17_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>To achieve peak performance, it is often necessary to adjust the parameters of a given algorithm to the class of problem instances to be solved; this is known to be the case for popular solvers for a broad range of AI problems, including AI planning, propositional satisfiability (SAT) and answer set programming (ASP). To avoid tedious and often highly sub-optimal manual tuning of such parameters by means of ad-hoc methods, general-purpose algorithm configuration procedures can be used to automatically find performance-optimizing parameter settings. While impressive performance gains are often achieved in this manner, additional, potentially costly parameter importance analysis is required to gain insights into what parameter changes are most responsible for those improvements. Here, we show how the running time cost of ablation analysis, a well-known general-purpose approach for assessing parameter importance, can be reduced substantially by using regression models of algorithm performance constructed from data collected during the configuration process. In our experiments, we demonstrate speed-up factors between 33 and 14 727 for ablation analysis on various configuration scenarios from AI planning, SAT, ASP and mixed integer programming (MIP).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">biedenkapp-aaai17a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient Parameter Importance Analysis via Ablation with Surrogates}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Biedenkapp, Andr√© and Lindauer, Marius and Eggensperger, Katharina and Fawcett, Chris and Hoos, Holger H and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Thirty-First Conference on Artificial Intelligence (AAAI'17)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{773--779}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> ¬© Copyright 2026 Andr√© Biedenkapp. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> &amp; <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a>. Last updated: Feb 09, 2026. <a href="https://andrebiedenkapp.github.io/impressum/">Impressum</a>. <a href="https://andrebiedenkapp.github.io/privacy-policy/">Privacy Policy</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>