<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Andr√© Biedenkapp</title> <meta name="author" content="Andr√© Biedenkapp"/> <meta name="description" content="Personal Website of Andr√© Biedenkapp "/> <meta name="keywords" content="Dynamic Algorithm Configuration, Reinforcement Learning, Learning to Learn"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü§ñ</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://andrebiedenkapp.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%62%69%65%64%65%6E%6B%61@%63%73.%75%6E%69-%66%72%65%69%62%75%72%67.%64%65" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=WvtpDmcAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://orcid.org/0000-0002-8703-8559" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a> <a href="https://github.com/AndreBiedenkapp" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://dblp1.uni-trier.de/pid/195/8188.html" title="DBLP" target="_blank" rel="noopener noreferrer"><i class="ai ai-dblp"></i></a> <a href="https://ml.informatik.uni-freiburg.de/~biedenka" title="Work" target="_blank" rel="noopener noreferrer"><i class="fas fa-briefcase"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link text-lowercase" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link text-lowercase" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link text-lowercase" href="/teaching/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link text-lowercase" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Andr√©</span> Biedenkapp </h1> <p class="desc"><a href="https://ml.informatik.uni-freiburg.de/focus-groups/reinforcement-learning/" target="_blank" rel="noopener noreferrer">RL Subgroup Leader</a> | <a href="https://ml.informatik.uni-freiburg.de/" target="_blank" rel="noopener noreferrer">Machine Learning Lab</a> | <a href="https://uni-freiburg.de/" target="_blank" rel="noopener noreferrer">University of Freiburg</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/andre_defense_headshot-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/andre_defense_headshot-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/andre_defense_headshot-1400.webp"></source> <img src="/assets/img/andre_defense_headshot.png" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="andre_defense_headshot.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>Room 00-018</p> <p>Georges-K√∂hler-Allee 74</p> <p>79110 Freiburg, Germany</p> <p style="margin: 10px 0 0 0; font-size: 75% !important">My 3 word address: <a href="https://what3words.com/forecast.gamer.showcase" target="_blank" rel="noopener noreferrer">///forecast.gamer.showcase</a></p> </div> </div> <div class="clearfix"> <p>I lead the <a href="https://ml.informatik.uni-freiburg.de/focus-groups/reinforcement-learning/" target="_blank" rel="noopener noreferrer">Reinforcement Learning subgroup</a> at the University of Freiburg‚Äôs Machine Learning Lab, where I develop algorithms that learn generalist policies. My research has been recognized with best paper awards at GECCO and AutoML, and my work on Dynamic Algorithm Configuration has pioneered new approaches for adaptive algorithmic systems.</p> <p><strong>Affiliations:</strong></p> <ul> <li>Member of <a href="https://ellis.eu/" target="_blank" rel="noopener noreferrer">ELLIS</a><br>(European Laboratory for Learning and Intelligent Systems)</li> <li>Co-Founder of <a href="https://autorl.org/" target="_blank" rel="noopener noreferrer">AutoRL.org</a> </li> <li>Chair of <a href="https://www.coseal.net/" target="_blank" rel="noopener noreferrer">COSEAL.net</a> </li> </ul> <p>I completed my education at the University of Freiburg, earning my B.Sc. (2015) and M.Sc. (2017) in Computer Science. In October 2022, I received my Ph.D. (Dr. rer. nat.) under the joint supervision of Prof. Dr. Frank Hutter and Prof. Dr. Marius Lindauer (Leibniz University Hannover), with a thesis on <a href="https://ml.informatik.uni-freiburg.de/wp-content/uploads/2022/11/2022_Dissertation_Andre_Biedenkapp.pdf" target="_blank" rel="noopener noreferrer">Dynamic Algorithm Configuration by Reinforcement Learning</a>.</p> </div> <hr> <div class="news"> <h2>news</h2> <div class="table-responsive" style="max-height: 22vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jul 18, 2025</th> <td> Our GECCO‚Äô25 paper <a href="https://arxiv.org/abs/2502.20265" target="_blank" rel="noopener noreferrer">On the Importance of Reward Design in Reinforcement Learning-based Dynamic Algorithm Configuration: A Case Study on OneMax with (1+(Lambda,Lambda))-GA</a> <em><a href="https://gecco-2022.sigevo.org/Best-Paper-Nominations" target="_blank" rel="noopener noreferrer"><span style="color: #F29105 !important">won the best paper award on the L4EC track</span></a></em>. </td> </tr> <tr> <th scope="row">Jul 18, 2025</th> <td> <strong>Two papers accepted at EWRL 2025:</strong> ‚ÄúOne Does Not Simply Estimate State: Comparing Model-based and Model-free Reinforcement Learning on the Partially Observable MordorHike Benchmark‚Äù introduces a new benchmark for rigorous state estimation testing in POMDPs, and ‚ÄúMighty: A Comprehensive Tool for studying Generalization, Meta-RL and AutoRL‚Äù presents an open-source library unifying contextual generalization, meta-learning, and automated hyperparameter optimization. Looking forward to discussing these papers in T√ºbingen! </td> </tr> <tr> <th scope="row">May 22, 2025</th> <td> Our GECCO‚Äô25 paper <a href="https://arxiv.org/abs/2502.20265" target="_blank" rel="noopener noreferrer">On the Importance of Reward Design in Reinforcement Learning-based Dynamic Algorithm Configuration: A Case Study on OneMax with (1+(Œª,Œª))-GA</a> has been <em><span style="color: #F29105 !important">nominated for the best paper award at this year‚Äôs GECCO</span></em>. </td> </tr> <tr> <th scope="row">Mar 19, 2025</th> <td> Our paper ‚Äú<a href="https://arxiv.org/abs/2502.20265" target="_blank" rel="noopener noreferrer">On the Importance of Reward Design in RL-based Dynamic Algorithm Configuration</a>‚Äù has been accepted at GECCO 2025. We demonstrate how adaptive reward shifting overcomes scalability issues in DAC, achieving superior performance with significantly fewer training steps when controlling the (1+(Œª,Œª))-GA algorithm. </td> </tr> <tr> <th scope="row">Mar 11, 2025</th> <td> <a href="https://openreview.net/forum?id=d9htascfP8" target="_blank" rel="noopener noreferrer">Meta-learning Population-based Methods for Reinforcement Learning</a> has been accepted in TMLR. We address the slow start problem in PB2 with novel meta-learning approaches that leverage cross-environment knowledge. Our MultiTaskPB2 demonstrates superior performance across diverse RL benchmarks. </td> </tr> <tr> <th scope="row">Jan 22, 2025</th> <td> Our paper on <a href="https://openreview.net/forum?id=UENQuayzr1" target="_blank" rel="noopener noreferrer">Efficient Cross-Episode Meta-RL</a>, introducing ECET (Efficient Cross-Episodic Transformers) was accepted at ICLR 2025. ECET presents a novel hierarchical transformer architecture for meta-RL that significantly improves generalization while maintaining efficient meta-training, demonstrated across MuJoCo, Meta-World and ManiSkill benchmarks. </td> </tr> <tr> <th scope="row">Sep 12, 2024</th> <td> Our AutoML‚Äô24 paper <a href="https://openreview.net/forum?id=MlB61zPAeR" target="_blank" rel="noopener noreferrer">HPO-RL-Bench: A Zero-Cost Benchmark for HPO in Reinforcement Learning</a> <em><a href="https://2024.automl.cc/?page_id=1406" target="_blank" rel="noopener noreferrer"><span style="color: #F29105 !important">was awarded runner up for the best paper award!</span></a></em> </td> </tr> <tr> <th scope="row">Aug 1, 2024</th> <td> We have two new RL papers accepted at the <a href="https://ewrl.wordpress.com/ewrl17-2024/" target="_blank" rel="noopener noreferrer">European Workshop on Reinforcement Learning (EWRL‚Äô24)</a>. I hope to see you there! </td> </tr> <tr> <th scope="row">Jul 12, 2024</th> <td> I‚Äôm happy to contribute to this years <a href="https://2024.automl.cc/" target="_blank" rel="noopener noreferrer">AutoML conference</a> with a workshop paper and a tutorial. The <a href="https://arxiv.org/abs/2407.05789" target="_blank" rel="noopener noreferrer">paper</a> discusses how to exploit structure in the configuration space for DAC by RL. The tutorial will be held jointly with <a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a> on AutoRL. Looking forward to seeing you there! </td> </tr> <tr> <th scope="row">Jun 7, 2024</th> <td> <a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a> and I will be giving a tutorial on AutoRL with a focus on applications to sustainability this September at the AutoML School 2024. I‚Äôm looking forward to seeing you there! You can find more information <a href="https://www.automlschool.org/home" target="_blank" rel="noopener noreferrer">here</a>. </td> </tr> <tr> <th scope="row">May 15, 2024</th> <td> Our research on improving zero-shot generalization of world models for Contextual RL has been accepted at the inaugural <a href="https://rl-conference.cc/index.html" target="_blank" rel="noopener noreferrer">Reinforcement Learning Conference (RLC)</a>. You can find the preprint of the paper <a href="https://arxiv.org/abs/2403.10967" target="_blank" rel="noopener noreferrer">here</a>. You should also check out Sai‚Äôs twitter thread on the paper <a href="https://twitter.com/sai_prasanna/status/1791121630187454596" target="_blank" rel="noopener noreferrer">here</a>. </td> </tr> <tr> <th scope="row">Apr 3, 2024</th> <td> I‚Äôm co-orgainzing the <a href="https://autorlworkshop.github.io/" target="_blank" rel="noopener noreferrer">AutoRL Workshop</a> at ICML 2024. The workshop will be held on July 26th or 27th (to be confirmed). The workshop will feature invited talks, contributed talks, and a poster session. The call for papers will be out soon. I‚Äôm looking forward to seeing you there! </td> </tr> <tr> <th scope="row">Mar 15, 2024</th> <td> Jointly with <a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a> I will be giving a tutorial on AutoRL this October at ECAI 2024. I‚Äôm looking forward to seeing you there! You can find more information <a href="https://www.ecai2024.eu/programme/tutorials/beyond-trial-error-a-tutorial-on-automated-reinforcement-learning" target="_blank" rel="noopener noreferrer">here</a>. </td> </tr> <tr> <th scope="row">Feb 12, 2024</th> <td> In our <a href="https://arxiv.org/abs/2402.06402" target="_blank" rel="noopener noreferrer">latest paper</a> we propose that attention-based Meta-RL can achieve improved generalization capabilities by learning transition dynamics as well as learning dynamics by paying attention to intra- as well as inter-episode experiences. </td> </tr> <tr> <th scope="row">Jan 11, 2024</th> <td> Raghu Rajan, Theresa Eimer, Aditya Mohan and I have written a blog post on the AutoRL blog about the year 2023 in AutoRL research. You can read it <a href="http://autorl.org/blog/retrospective/" target="_blank" rel="noopener noreferrer">here</a>. </td> </tr> <tr> <th scope="row">Aug 17, 2023</th> <td> September will be busy. Our (Auto)RL works will be presented in two upcoming venues: <ul> <li> <a href="https://twitter.com/RaghuSpaceRajan" target="_blank" rel="noopener noreferrer">Raghu Rajan</a> will present our <a href="https://jair.org/index.php/jair/article/view/13596" target="_blank" rel="noopener noreferrer">AutoRL survey</a> at the <a href="https://2023.automl.cc/" target="_blank" rel="noopener noreferrer">AutoML 2023</a> conference.</li> <li> <a href="https://twitter.com/The_Eimer" target="_blank" rel="noopener noreferrer">Theresa Eimer</a> will present our <a href="https://openreview.net/forum?id=Y42xVBQusn" target="_blank" rel="noopener noreferrer">benchmark CARL</a> at the <a href="https://ewrl.wordpress.com/ewrl16-2023" target="_blank" rel="noopener noreferrer">EWRL 2023</a> workshop.</li> </ul> Make sure you come by and say hi! </td> </tr> <tr> <th scope="row">Jul 4, 2023</th> <td> Our latest paper presenting the <em><a href="https://jair.org/index.php/jair/article/view/14314" target="_blank" rel="noopener noreferrer">MDP Playground</a></em> has been accepted published by JAIR. With the MDPP, you can create and control your own <strong>cheap but challenging RL environments</strong> to better analyze and understand the behavior of RL algorithms. </td> </tr> <tr> <th scope="row">Jun 5, 2023</th> <td> Our latest TMLR paper on <strong>contextual RL</strong> discusses how cRL allows principled study on generalization in RL and proposes a flexible benchmark suite for cRL.<br> Check out the paper <a href="https://openreview.net/forum?id=Y42xVBQusn" target="_blank" rel="noopener noreferrer">here</a> and the benchmark <a href="https://github.com/automl/CARL" target="_blank" rel="noopener noreferrer">here</a>. </td> </tr> <tr> <th scope="row">Mar 10, 2023</th> <td> I‚Äôll be chairing the online experience of this year‚Äôs <a href="https://2023.automl.cc/organizers/" target="_blank" rel="noopener noreferrer">AutoML Conference</a> jointly with Hayeon Lee, Mohamed Abdelfattah and Richard Song. Checkout the blog post <a href="https://2023.automl.cc/blog/hybrid-conference/" target="_blank" rel="noopener noreferrer">here</a> for more details. </td> </tr> <tr> <th scope="row">Jan 30, 2023</th> <td> Our paper proposing a novel gray-box BO technique for <strong>AutoRL</strong> was accepted at <a href="https://iclr.cc/virtual/2023/poster/10730" target="_blank" rel="noopener noreferrer">ICLR‚Äô23</a>. </td> </tr> <tr> <th scope="row">Dec 1, 2022</th> <td> Our <a href="https://www.jair.org/index.php/jair/article/view/13922" target="_blank" rel="noopener noreferrer">recent paper</a> in which we discuss the past, present and future of <strong>dynamic algorithm configuration (DAC)</strong> has been accepted by the Journal for Artificial Intelligence Research. </td> </tr> <tr> <th scope="row">Nov 11, 2022</th> <td> We have two new workshop papers tackling <strong>AutoRL</strong> accepted at <a href="https://meta-learn.github.io/2022/" target="_blank" rel="noopener noreferrer">MetaLearn@NeurIPS‚Äô22</a>:<br> <ul> <li> <a href="https://openreview.net/forum?id=RyAl60VhTcG" target="_blank" rel="noopener noreferrer">AutoRL-Bench 1.0</a> introduces a tabular benchmark that enables very fast, fair, and reproducible experimental protocols for comparing future HPO methods for RL.<br> </li> <li> <a href="">Gray-Box Gaussian Processes for Automated Reinforcement Learning</a> introduces a novel gray-box Bayesian Optimization technique for HPO in RL.</li> </ul> </td> </tr> <tr> <th scope="row">Oct 14, 2022</th> <td> üéâ I successfully defended my PhD thesis with the title <strong>Dynamic Algorithm Configuration by Reinforcement Learning</strong> with <em><span style="color: #F29105 !important">summa cum laude</span></em> (the best possible grade) ü•≥ </td> </tr> <tr> <th scope="row">Sep 20, 2022</th> <td> On the 10th of November I‚Äôll be giving a talk about my research in the <a href="https://aaltoml.github.io/apml/" target="_blank" rel="noopener noreferrer">Seminar on Advances in Probabilistic Machine Learning</a> of the Aalto University and ELLIS unit Helsinki. </td> </tr> <tr> <th scope="row">Aug 11, 2022</th> <td> I am Chair of <a href="https://www.coseal.net/" target="_blank" rel="noopener noreferrer">COSEAL</a> jointly with Alexander Tornede and Lennart Sh√§permeier. You can find the announcement <a href="https://twitter.com/coseal_net/status/1557708325457903616?s=20&amp;t=pfdPWJ99xDLxb7h_4tsFLg" target="_blank" rel="noopener noreferrer">here</a>. </td> </tr> <tr> <th scope="row">Jul 13, 2022</th> <td> Our GECCO‚Äô22 paper <a href="https://arxiv.org/abs/2202.03259" target="_blank" rel="noopener noreferrer">Theory-Inspired Parameter Control Benchmarks for Dynamic Algorithm Configuration</a> <em><a href="https://gecco-2022.sigevo.org/Best-Paper-Nominations" target="_blank" rel="noopener noreferrer"><span style="color: #F29105 !important">won the best paper award on the GECH track</span></a></em>. </td> </tr> <tr> <th scope="row">Jun 24, 2022</th> <td> We just uploaded the talk for ou GECCO‚Äô22 paper <a href="https://youtu.be/xgljDu5qE-w" target="_blank" rel="noopener noreferrer">Theory-Inspired Parameter Control Benchmarks for Dynamic Algorithm Configuration</a> which is <em><a href="https://gecco-2022.sigevo.org/Best-Paper-Nominations" target="_blank" rel="noopener noreferrer">nominated for the best paper award</a></em>. </td> </tr> <tr> <th scope="row">Jun 19, 2022</th> <td> Checkout our <a href="https://arxiv.org/abs/2206.03493" target="_blank" rel="noopener noreferrer">new paper on DeepCAVE</a>, a tool to analyze and explain AutoML meta-data, which was just accepted at the <a href="https://realworldml.github.io/" target="_blank" rel="noopener noreferrer">ReALML@ICML workshop</a>. </td> </tr> <tr> <th scope="row">Jun 3, 2022</th> <td> I am co-organizing the <a href="https://sites.google.com/view/automl-fall-school-2022/home" target="_blank" rel="noopener noreferrer">2nd AutoML Fall School</a>. The fall school will be held from 10th - 13th of October. Checkout the exciting list of invited speakers and hands-on sessions. </td> </tr> <tr> <th scope="row">Jun 3, 2022</th> <td> We just released <a href="https://andrebiedenkapp.github.io/assets/pdf/22-PRL-DAC4AIPlanning.pdf">a new paper</a> on DAC for AI Planning. I‚Äôll present the paper on the 13th of June at the <a href="https://prl-theworkshop.github.io/prl2022-icaps/" target="_blank" rel="noopener noreferrer">PRL Workshop</a>. You can also checkout the <a href="https://youtu.be/RgyYaJIr4p8" target="_blank" rel="noopener noreferrer">recorded talk</a>. </td> </tr> <tr> <th scope="row">Jun 1, 2022</th> <td> Our <a href="https://andrebiedenkapp.github.io/assets/pdf/paper/22-JAIR-AutoRL.pdf">survey on AutoRL</a> has been published in the <a href="https://jair.org/index.php/jair/article/view/13596" target="_blank" rel="noopener noreferrer">Journal of Artificial Intelligence Research</a>. </td> </tr> <tr> <th scope="row">May 30, 2022</th> <td> We just released <a href="https://arxiv.org/abs/2205.13881" target="_blank" rel="noopener noreferrer">a new paper</a> on Dynamic Algorithm Configuration (DAC) in which we discuss the DAC journey so far and give a glimpse of the future of DAC. </td> </tr> <tr> <th scope="row">May 18, 2022</th> <td> Our GECCO‚Äô22 paper <a href="https://arxiv.org/abs/2202.03259" target="_blank" rel="noopener noreferrer">Theory-Inspired Parameter Control Benchmarks for Dynamic Algorithm Configuration</a> has been <em><a href="https://gecco-2022.sigevo.org/Best-Paper-Nominations" target="_blank" rel="noopener noreferrer">nominated for the best paper award</a></em>. </td> </tr> <tr> <th scope="row">Apr 16, 2022</th> <td> The paper <a href="https://arxiv.org/abs/2202.03259" target="_blank" rel="noopener noreferrer">Theory-Inspired Parameter Control Benchmarks for Dynamic Algorithm Configuration</a> is accepted for publication in the proceedings of the Genetic and Evolutionary Computation Conference (GECCO‚Äô22). </td> </tr> <tr> <th scope="row">Apr 16, 2022</th> <td> I created this personal webpage <img class="emoji" title=":grin:" alt=":grin:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f601.png" height="20" width="20"> </td> </tr> </table> </div> </div> <hr> <div class="publications"> <h2>selected publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="prasanna-rlc24a" class="col-sm-10"> <div class="title">Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization</div> <div class="author"> Sai Prasanna*,¬†Karim Farid*,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/rajan/" target="_blank" rel="noopener noreferrer">Raghu Rajan</a>,¬†and¬†<b>Andr√© Biedenkapp</b> </div> <div class="periodical"> <em>Reinforcement Learning Journal</em>, 3, pp. 1317‚Äì1350, 2024 <small><b>*Joint first authorship</b></small><br> <small><span style="color: #82828299">Note: To also be presented at the Seventeenth European Workshop on Reinforcement Learning (EWRL 2024)</span></small> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/24_rlc_dreaming_of_many_worlds.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://openreview.net/forum?id=o8DrRuBsQb" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a> <a href="http://arxiv.org/abs/arXiv:2403.10967" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/sai-prasanna/dreaming_of_many_worlds" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster/DOMW_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://rlj.cs.umass.edu/2024/papers/Paper167.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents. To address the broader challenge, we start with the simpler setting of contextual reinforcement learning (cRL), assuming observability of the context values that parameterize the variation in the system‚Äôs dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state. Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of the Dreamer (v3) (Hafner et al., 2023). This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics. Our experiments show that such systematic incorporation of the context improves the ZSG of the policies trained on the ‚Äúdreams‚Äù of the world model. We further find qualitatively that our approach allows Dreamer to disentangle the latent state from context, allowing it to extrapolate its dreams to the many worlds of unseen contexts. The code for all our experiments is available at https://github.com/sai-prasanna/dreaming_of_many_worlds.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">prasanna-rlc24a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Prasanna, Sai and Farid, Karim and Rajan, Raghu and Biedenkapp, Andr√©}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Reinforcement Learning Journal}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">issue</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1317--1350}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="biedenkapp-gecco22a" class="col-sm-10"> <div class="title">Theory-inspired Parameter Control Benchmarks for Dynamic Algorithm Configuration</div> <div class="author"> <b>Andr√© Biedenkapp*</b>,¬†<a href="https://www.st-andrews.ac.uk/computer-science/people/nttd/" target="_blank" rel="noopener noreferrer">Nguyen Dang*</a>,¬†Martin S. Krejca*,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="http://www-ia.lip6.fr/~doerr/index.html" target="_blank" rel="noopener noreferrer">Carola Doerr</a> </div> <div class="periodical"> <em>In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO‚Äô22)</em>, pp. 766‚Äì775, 2022 <small><b>*Joint first authorship</b></small> <small>üèÖ<span style="color: #F29105 !important">Won the best paper award on the <a href="https://gecco-2022.sigevo.org/Best-Paper-Awards#GECH_Track" target="_blank" rel="noopener noreferrer">GECH track.</a></span></small><br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/22-gecco.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="http://arxiv.org/abs/arXiv:2202.03259" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://andrebiedenkapp.github.io/blog/2022/gecco/" class="btn btn-sm z-depth-0" role="button">Blog</a> <a href="https://github.com/ndangtt/LeadingOnesDAC" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.automl.org/automated-algorithm-design/dac/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="https://youtu.be/xgljDu5qE-w" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>It has long been observed that the performance of evolutionary algorithms and other randomized search heuristics can benefit from a non-static choice of the parameters that steer their optimization behavior. Mechanisms that identify suitable configurations on the fly ("parameter control") or via a dedicated training process ("dynamic algorithm configuration") are therefore an important component of modern evolutionary computation frameworks. Several approaches to address the dynamic parameter setting problem exist, but we barely understand which ones to prefer for which applications. As in classical benchmarking, problem collections with a known ground truth can offer very meaningful insights in this context. Unfortunately, settings with well-understood control policies are very rare. One of the few exceptions for which we know which parameter settings minimize the expected runtime is the LeadingOnes problem. We extend this benchmark by analyzing optimal control policies that can select the parameters only from a given portfolio of possible values. This also allows us to compute optimal parameter portfolios of a given size. We demonstrate the usefulness of our benchmarks by analyzing the behavior of the DDQN reinforcement learning approach for dynamic algorithm configuration.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">biedenkapp-gecco22a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Theory-inspired Parameter Control Benchmarks for Dynamic Algorithm Configuration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Biedenkapp, Andr√© and Dang, Nguyen and Krejca, Martin S. and Hutter, Frank and Doerr, Carola}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{766--775}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Genetic and Evolutionary Computation Conference ({GECCO}'22)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="parker-holder-jair22a" class="col-sm-10"> <div class="title">Automated Reinforcement Learning (AutoRL): A Survey and Open Problems</div> <div class="author"> Jack Parker-Holder,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/rajan/" target="_blank" rel="noopener noreferrer">Raghu Rajan</a>,¬†Xingyou Song,¬†<b>Andr√© Biedenkapp</b>,¬†Yingjie Miao,¬†<a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a>,¬†Baohe Zhang,¬†Vu Nguyen,¬†Roberto Calandra,¬†Aleksandra Faust,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>Journal of Artificial Intelligence Research (JAIR)</em>, 74, pp. 517-568, 2022<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/22-JAIR-AutoRL.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="http://arxiv.org/abs/arXiv:2201.03916" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="/assets/pdf/poster/AutoRL_Survey_JAIR_2022_building_74.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://www.youtube.com/watch?v=6TcSGstpSUE&amp;list=PLp7L30nGpKM8LnCrbnT81KjBn5uxB6u8n&amp;index=3" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>The combination of Reinforcement Learning (RL) with deep learning has led to a series of impressive feats, with many believing (deep) RL provides a path towards generally capable agents. However, the success of RL agents is often highly sensitive to design choices in the training process, which may require tedious and error-prone manual tuning. This makes it challenging to use RL for new problems, while also limits its full potential. In many other areas of machine learning, AutoML has shown it is possible to automate such design choices and has also yielded promising initial results when applied to RL. However, Automated Reinforcement Learning (AutoRL) involves not only standard applications of AutoML but also includes additional challenges unique to RL, that naturally produce a different set of methods. As such, AutoRL has been emerging as an important area of research in RL, providing promise in a variety of applications from RNA design to playing games such as Go. Given the diversity of methods and environments considered in RL, much of the research has been conducted in distinct subfields, ranging from meta-learning to evolution. In this survey we seek to unify the field of AutoRL, we provide a common taxonomy, discuss each area in detail and pose open problems which would be of interest to researchers going forward.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">parker-holder-jair22a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automated Reinforcement Learning (AutoRL): A Survey and Open Problems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Parker-Holder, Jack and Rajan, Raghu and Song, Xingyou and Biedenkapp, Andr√© and Miao, Yingjie and Eimer, Theresa and Zhang, Baohe and Nguyen, Vu and Calandra, Roberto and Faust, Aleksandra and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Artificial Intelligence Research (JAIR)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{517-568}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{74}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1613/jair.1.13596}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="biedenkapp-icml21" class="col-sm-10"> <div class="title">TempoRL: Learning When to Act</div> <div class="author"> <b>Andr√© Biedenkapp</b>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/rajan/" target="_blank" rel="noopener noreferrer">Raghu Rajan</a>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>In Proceedings of the 38th International Conference on Machine Learning (ICML 2021)</em>, 139, pp. 914‚Äì924, 2021<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/biedenkapp21a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/paper/biedenkapp21a-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="http://arxiv.org/abs/2106.05262" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://andrebiedenkapp.github.io/blog/2022/temporl" class="btn btn-sm z-depth-0" role="button">Blog</a> <a href="https://github.com/automl/TempoRL" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster/tempoRL_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/slides/2021_TempoRL@ICML.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://slideslive.com/38959338/temporl-learning-when-to-act" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>Reinforcement learning is a powerful approach to learn behaviour through interactions with an environment. However, behaviours are usually learned in a purely reactive fashion, where an appropriate action is selected based on an observation. In this form, it is challenging to learn when it is necessary to execute new decisions. This makes learning inefficient, especially in environments that need various degrees of fine and coarse control. To address this, we propose a proactive setting in which the agent not only selects an action in a state but also for how long to commit to that action. Our TempoRL approach introduces skip connections between states and learns a skip-policy for repeating the same action along these skips. We demonstrate the effectiveness of TempoRL on a variety of traditional and deep RL environments, showing that our approach is capable of learning successful policies up to an order of magnitude faster than vanilla Q-learning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">biedenkapp-icml21</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TempoRL: Learning When to Act}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Biedenkapp, Andr√© and Rajan, Raghu and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{139}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{914--924}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 38th International Conference on Machine Learning (ICML 2021)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="biedenkapp-ecai20" class="col-sm-10"> <div class="title">Dynamic Algorithm Configuration: Foundation of a New Meta-Algorithmic Framework</div> <div class="author"> <b>Andr√© Biedenkapp</b>,¬†Furkan H Bozkurt,¬†<a href="https://theeimer.github.io/" target="_blank" rel="noopener noreferrer">Theresa Eimer</a>,¬†<a href="https://ml.informatik.uni-freiburg.de/profile/hutter" target="_blank" rel="noopener noreferrer">Frank Hutter</a>,¬†and¬†<a href="https://www.tnt.uni-hannover.de/staff/lindauer/" target="_blank" rel="noopener noreferrer">Marius Lindauer</a> </div> <div class="periodical"> <em>In Proceedings of the Twenty-fourth European Conference on Artificial Intelligence (ECAI‚Äô20)</em>, pp. 427‚Äì434, 2020<br> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/paper/20-ecai-dac.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/automl/DAC/blob/master/Appendix.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a> <a href="https://www.automl.org/dynamic-algorithm-configuration/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a> <a href="https://github.com/automl/DAC" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/slides/2020_DAC@ECAI.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://www.youtube.com/watch?v=wxPYtSGT05s&amp;feature=youtu.be" class="talk btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a> </div> <div class="abstract hidden"> <p>The performance of many algorithms in the fields of hard combinatorial problem solving, machine learning or AI in general depends on parameter tuning. Automated methods have been proposed to alleviate users from the tedious and error-prone task of manually searching for performance-optimized configurations across a set of problem instances. However, there is still a lot of untapped potential through adjusting an algorithm‚Äôs parameters online since different parameter values can be optimal at different stages of the algorithm. Prior work showed that reinforcement learning is an effective approach to learn policies for online adjustments of algorithm parameters in a data-driven way. We extend that approach by formulating the resulting dynamic algorithm configuration as a contextual MDP, such that RL not only learns a policy for a single instance, but across a set of instances. To lay the foundation for studying dynamic algorithm configuration with RL in a controlled setting, we propose white-box benchmarks covering major aspects that make dynamic algorithm configuration a hard problem in practice and study the per- formance of various types of configuration strategies for them. On these white-box benchmarks, we show that (i) RL is a robust candidate for learning configuration policies, outperforming standard pa- rameter optimization approaches, such as classical algorithm configuration; (ii) based on function approximation, RL agents can learn to generalize to new types of instances; and (iii) self-paced learning can substantially improve the performance by selecting a useful sequence of training instances automatically.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">biedenkapp-ecai20</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dynamic Algorithm Configuration: Foundation of a New Meta-Algorithmic Framework}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Biedenkapp, Andr√© and Bozkurt, Furkan H and Eimer, Theresa and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{427--434}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Twenty-fourth European Conference on Artificial Intelligence (ECAI'20)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <hr> <div class="social"> <div class="contact-icons"> <a href="mailto:%62%69%65%64%65%6E%6B%61@%63%73.%75%6E%69-%66%72%65%69%62%75%72%67.%64%65" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=WvtpDmcAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://orcid.org/0000-0002-8703-8559" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a> <a href="https://github.com/AndreBiedenkapp" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://dblp1.uni-trier.de/pid/195/8188.html" title="DBLP" target="_blank" rel="noopener noreferrer"><i class="ai ai-dblp"></i></a> <a href="https://ml.informatik.uni-freiburg.de/~biedenka" title="Work" target="_blank" rel="noopener noreferrer"><i class="fas fa-briefcase"></i></a> </div> <div class="contact-note"> Feel free to drop me a line via email. </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> ¬© Copyright 2025 Andr√© Biedenkapp. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> &amp; <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a>. Last updated: Oct 06, 2025. <a href="https://andrebiedenkapp.github.io/impressum/">Impressum</a>. <a href="https://andrebiedenkapp.github.io/privacy-policy/">Privacy Policy</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>