<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://andrebiedenkapp.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://andrebiedenkapp.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-06T07:25:00+00:00</updated><id>https://andrebiedenkapp.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal Website of André Biedenkapp </subtitle><entry><title type="html">2023 in AutoRL</title><link href="https://andrebiedenkapp.github.io/blog/2024/AutoRL-redirect/" rel="alternate" type="text/html" title="2023 in AutoRL"/><published>2024-01-11T10:00:00+00:00</published><updated>2024-01-11T10:00:00+00:00</updated><id>https://andrebiedenkapp.github.io/blog/2024/AutoRL-redirect</id><content type="html" xml:base="https://andrebiedenkapp.github.io/blog/2024/AutoRL-redirect/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[A retrospective on 2023 in AutoRL research written by a collective of AutoRL researchers. Redirects to the post on the AutoRL blog.]]></summary></entry><entry><title type="html">On Hybrid Conferences</title><link href="https://andrebiedenkapp.github.io/blog/2023/AutoML-redirect/" rel="alternate" type="text/html" title="On Hybrid Conferences"/><published>2023-03-15T12:00:00+00:00</published><updated>2023-03-15T12:00:00+00:00</updated><id>https://andrebiedenkapp.github.io/blog/2023/AutoML-redirect</id><content type="html" xml:base="https://andrebiedenkapp.github.io/blog/2023/AutoML-redirect/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Blog post announcing the online experience chairs of the AutoML Conference 2023.]]></summary></entry><entry><title type="html">TempoRL - Learning When to Act</title><link href="https://andrebiedenkapp.github.io/blog/2022/temporl/" rel="alternate" type="text/html" title="TempoRL - Learning When to Act"/><published>2022-05-27T00:00:00+00:00</published><updated>2022-05-27T00:00:00+00:00</updated><id>https://andrebiedenkapp.github.io/blog/2022/temporl</id><content type="html" xml:base="https://andrebiedenkapp.github.io/blog/2022/temporl/"><![CDATA[<p><strong>NOTE:</strong> We are using <a href="https://pyscript.net/"><font color="lightblue">pyscript</font></a> for the example below. Loading might take a bit longer.</p> <div id="pyexample" style="border: 2px solid #ccc !important; border-radius: 5px; padding: 5px 5px 5px 5px;"> <canvas id="envcanvas" style="background-color: black; alignment: center; width: 100%; height=20%"> Loading... </canvas> <div id="plot"></div> <py-script src="/assets/python_scripts/2022-06-01-temporl/temporl.py" output="plot"></py-script> <div class="input-group"> <span class="input-group-btn" style="width: 50%"> <label style="margin-left: 10px;"> $\color{gray}\color{gray}\mathcal{Q}$-learning:<br/><label><input type="radio" id="tempo-radio" name="Admin" style="margin-right: 5px"/>vanilla</label> <input type="radio" id="vanilla-radio" name="Admin" checked="" style="display:inline-block; margin-right: 5px; margin-left: 10px"/>TempoRL</label> <label for="quantity" style="margin-left: 10px;">Skip: </label> <input type="number" id="quantity" name="quantity" min="2" max="10" value="7" style="background-color: #828282; width: 15%; text-align:center; margin-left: 5px"/><br/> <button type="button" class="btn btn-default" id="start-btn" style="alignment: center; min-width: 40%">(RE-)START</button> <button type="button" class="btn btn-default" id="stop-btn" style="alignment: center; min-width: 40%">STOP</button><br/> <input type="range" min="1" max="100" value="50" step="1" class="slider" id="SpeedSlider" style=" -webkit-appearance: none; width: 25%; height: 10px; background: #d3d3d3; outline: none; opacity: 0.7; -webkit-transition: .2s; transition: opacity .2s; display: inline-block; margin-right: 10px; margin-left: 10px; " oninput="this.nextElementSibling.value = this.value"/> Eval Speed: <output>50</output>%<br/> <input type="range" min="0" max="1000" value="10" step="1" class="slider" id="LambdaSlider" style=" -webkit-appearance: none; width: 25%; height: 10px; background: #d3d3d3; outline: none; opacity: 0.7; -webkit-transition: .2s; transition: opacity .2s; display: inline-block; margin-right: 10px; margin-left: 10px; " oninput="this.nextElementSibling.value = this.value"/> Eval every <output>10</output> episode(s) </span> <span class="output-group"> <label for="envs" style="margin-right: 5px">Env: </label> <select name="environments" id="envs" style="background-color: #828282" onfocus="this.selectedIndex = -1;"> <option value="pit" checked="">Cliff</option> <option value="bridge">Bridge</option> <option value="zigzag">ZigZag</option> <option value="smallfield">EmptyField</option> <option value="largefield">EmptyField-XXL</option> </select><br/> Training Episodes: <div id="out-episodes" style="display: inline-block">-</div><br/> Avg. Train Reward: <div id="out-t-rew" style="display: inline-block">-</div><br/> <br/> Temporal Action: <div id="out-tempo" style="display: inline-block">-</div> </span> </div> </div> <div class="figcaption"> <p> TempoRL Demo using tabular agents. Play around to see how a TempoRL $\color{gray}\mathcal{Q}$-learning agent behaves compared to a vanilla one. Agents always start in the blue field and need to reach the orange field where they get a reward of $\color{gray}+1$. Falling down the cliff, i.e. black squares results in a reward of $\color{gray}-1$. Otherwise, the reward is always $\color{gray}0$. An episode is at most $\color{gray}100$ steps long. *Avg. Train Reward* shows the average training reward over the last $\color{gray}100$ episodes. The agents use a fixed $\color{gray}\epsilon$ of $\color{gray}0.1$. The maximal $\color{gray}\mathcal{Q}$-values are overlayed in green. The brighter the shade, the higher the value. </p> <br/> </div> <p>Reinforcement Learning (RL) is a powerful approach to train agents by letting them interact with their environment <d-cite key="rlbook"></d-cite>. Typically, this happens by letting the agent observe the current state of their environment. Based on this observation the agent then reacts, which results in an update to the environment which also might produce a reward or cost signal. The reward can then be used to reinforce desired behaviours, or, likewise, discourage bad behaviours. However, by only reacting to observations, agents do not learn <em>when</em> it is necessary to make a new decision. In our ICML’21 paper we explored a more proactive way of doing RL.</p> <h2 id="why-should-rl-agents-be-more-proactive">Why Should RL Agents be More Proactive?</h2> <p>An agent that does not only react to change in the environment, but actively anticipates what will happen, can quicker learn about consequences of their actions. This could improve learning speeds as agents would only need to focus on fewer critical decision points, rather than having to try and handle every observation the same. Further, a proactive agent is capable of more targeted exploration as the agent can commit to a plan of action for regions where it is certain until it requires replanning and exploration in less frequently visited areas. Finally, proactive agents are also more interpretable by not only stating which action to take in a state but also predicting when new decisions need to be made. This allows us to better understand the learned policies and, potentially, the underlying MDPs.</p> <p>Take a look at the example at the top of this post to verify these claims for yourself. The example provides simple tabular $\color{gray}\mathcal{Q}$-learning agents on environments with sparse rewards. If you select the <em>vanilla</em> version, you will train a standard agent. Our more proactive TempoRL agent can jointly learn how long an action should be repeated when it is played in a state. You can set the maximal repetition value. While all environments can be used to verify the claims above, you might observe the biggest differences on the <em>EmptyField</em> environment. Our TempoRL agent will quicker find a successful policy, by quicker backpropagation of the observed reward values. Further, the environment gets explored more thorough and the learned action repetition tells us that the agent views most states as equivalent such that it only needs to make few decisions to reach the goal.</p> <h2 id="how-to-train-proactive-rl-agents">How to Train Proactive RL Agents</h2> <p>To get to a more proactive way of RL, we proposed to jointly predict which action to take in a state and how long the action should be played. Our method TempoRL, counter to prior methods (see, e.g., <d-cite key="lakshminarayanan-aaai17,sharma-iclr17"></d-cite>) crucially conditions the repetition value in a state on the intended action. Thus, a TempoRL agent can target action repetition <em>only for those actions in a state that actually benefit from action repetition</em>. Further, through the use of action repetition we can quickly back propagate reward information. If we decide to repeat an action $\color{gray}N$ times, then we are able to learn about all smaller skips in between as well. Thus, we can very quickly observe if action repetitions is worth it and for how long an action should be repeated. An example of this is given in the following figure. <img src="/assets/img/blog/2022-06-01-temporl/skips.png" class="img-fluid rounded z-depth-1" data-zoomable="true" style="background-color: white; display: block; margin-left: auto; margin-right: auto; width: 50%;"/></p> <div class="figcaption"> <p> Observed action repetitions when committing to a larger repetition value. When repeating an action for 3 steps, we can also observe the value of repeating the same action for two steps (starting in different states) as well as playing the action only once (starting in different states). </p> </div> <p>Thus, we can quickly learn the repetition through n-step updates whereas we learn the action value through normal 1-step updates.</p> <h2 id="temporl">TempoRL</h2> <p>In this post we will spare you the details of how to implement TempoRL. To get an intuition of how TempoRL behaves we encourage you to go ahead and play with the demo on top. We suggest that you let the agents train for longer but frequently evaluate their performance to get an understanding for how the reward information is propagated. Quite quickly you might see that our TempoRL method finds a successful policy much quicker than the vanilla agent. You should see that, along the path of the successful policies, reward information is quickly back propagated and that TempoRL can then, over time, refine the policy to the optimal one. Counter to that, the vanilla agent, with its one-step exploration and backup, is only capable of slow backpropagation and explores a fairly small area.</p> <p>In the following we will show some of the results from our paper. However, this is only a brief summary of the results and there is much more to find in our paper.</p> <h3 id="tabular-agents">Tabular Agents</h3> <p><img src="/assets/img/blog/2022-06-01-temporl/cliff_example.png" class="img-fluid rounded z-depth-1" data-zoomable="true" style="background-color: white; display: block; margin-left: auto; margin-right: auto;"/></p> <div class="figcaption"> <p> Comparison of a Tabular $\color{#508d7c}\text{vanilla }\mathcal{Q}\text{-learning}$ agent vs. our $\color{#9f7499}\text{TempoRL }\mathcal{Q}\text{-learning}$ agent on the *Cliff* environment from above. Results are averaged over 100 random seeds. </p> </div> <p>We first evaluated TempoRL for tabular RL agents. The result of which you can play with at the beginning of this post. We observed large improvements in terms of learning speeds (see the previous figure for an example). Further, our results showed that TempoRL is robust to the choice of maximal repetition value (i.e. the skip value). However, the larger the skipping value, the more options our TempoRL agent needs to learn with. For much too large skipping values this can start to slow down learning before action repetition can be used effectively.</p> <h3 id="deep-rl-agents">Deep RL Agents</h3> <p>TempoRL is not limited to the tabular setting. To make it work in the deep case, we evaluated different architectures that make it possible to use TempoRL for featurized environments (i.e. environments with vector representation of states) as well as pixel-based environments. For details on the architectures we refer to our paper. As example, we trained the popular DQN <d-cite key="mnih-nature13"></d-cite> method on different Atari games.</p> <p><img src="/assets/img/blog/2022-06-01-temporl/qbert_blog.png" class="img-fluid rounded z-depth-1" data-zoomable="true" style="background-color: white; display: block; margin-left: auto; margin-right: auto;"/></p> <div class="figcaption"> <p> Comparison of a Tabular $\color{#508d7c}\text{DQN}$ agent vs. our $\color{#9f7499}\text{TempoRL DQN}$ agent on the Q*bert Atari environment as part of ALE<d-cite key="bellamare-jair13"></d-cite>. Results are averaged over 15 random seeds. The top plot gives the evaluation reward and the bottom plot the total number steps per evaluation episode (all) as well as the number of required decisions (dec). </p> </div> <p>On Q*bert<d-footnote>For details on the game see https://en.wikipedia.org/wiki/Q*bert</d-footnote> we could observe that TempoRL first needed to learn the value of repeating actions. Thus, in the beginning the performance of the TempoRL agent is lagging behind that of the vanilla DQN agent. Once TempoRL has learned to make correct use of action repetition however, it speeds up learning dramatically and outperforms the baseline agent.</p> <p><img src="/assets/img/blog/2022-06-01-temporl/freeway_blog.png" class="img-fluid rounded z-depth-1" data-zoomable="true" style="background-color: white; display: block; margin-left: auto; margin-right: auto;"/></p> <div class="figcaption"> <p> Comparison of a Tabular $\color{#508d7c}\text{DQN}$ agent vs. our $\color{#9f7499}\text{TempoRL DQN}$ agent on the freeway Atari environment as part of ALE<d-cite key="bellamare-jair13"></d-cite>. Results are averaged over 15 random seeds. The top plot gives the evaluation reward and the bottom plot the total number steps per evaluation episode (all) as well as the number of required decisions (dec). </p> </div> <p>A second environment worth mentioning is freeway<d-footnote>For details on the game see https://en.wikipedia.org/wiki/Freeway_(video_game)</d-footnote>. Here, an agent is tasked with clearing a busy freeway. To do so, agents will have to frequently repeat the <em>UP</em> action, only needing to stop to avoid collisions with oncoming cars. Our results show that in this environment our TempoRL method can drastically reduce the required number of decisions by the agent, thereby not only increasing the learning speed, but also learning better policies that reliably solve the environment, clearly outperforming the baseline agent.</p> <p>TempoRL however, is not only limited to $\color{gray}\mathcal{Q}$-learning methods. TempoRL can work with any value based agents. For example, in the paper we further evaluated TempoRL together with a DDPG agent. Again, for details we refer to the paper.</p> <h2 id="conclusion">Conclusion</h2> <p>We presented TempoRL, a method for more proactive RL agents. Our method can jointly learn <em>which</em> action to take and <em>when</em> it is necessary to make a new decision. We empirically evaluated our method using tabular and deep RL agents. In both settings we observed improved learning capabilities. We demonstrated that the improved learning speed not only comes from the ability of repeating actions but that the ability to learn which repetitions are helpful provided the basis of learning <em>when</em> to act. Our demo at the top of the post lets the reader confirm these claims themselves.</p> <p>This post is based on our ICML 2021 paper <em><a href="https://andrebiedenkapp.github.io/assets/pdf/biedenkapp21a.pdf"><font color="lightblue">TempoRL: Learning When to Act</font></a></em>. The code for the paper is available at <a href="https://github.com/automl/TempoRL"><font color="lightblue">https://github.com/automl/TempoRL</font></a>. If you find any issues in this post, please create an issue on <a href="https://github.com/AndreBiedenkapp/AndreBiedenkapp.github.io"><font color="lightblue">github</font></a>.</p>]]></content><author><name>André Biedenkapp</name></author><summary type="html"><![CDATA[Getting the best out of RL by learning when to act.]]></summary></entry><entry><title type="html">Theory-Inspired Parameter Control Benchmarks for DAC</title><link href="https://andrebiedenkapp.github.io/blog/2022/gecco/" rel="alternate" type="text/html" title="Theory-Inspired Parameter Control Benchmarks for DAC"/><published>2022-05-22T00:00:00+00:00</published><updated>2022-05-22T00:00:00+00:00</updated><id>https://andrebiedenkapp.github.io/blog/2022/gecco</id><content type="html" xml:base="https://andrebiedenkapp.github.io/blog/2022/gecco/"><![CDATA[<p><strong>NOTE:</strong> We are using <a href="https://pyscript.net/"><font color="lightblue">pyscript</font></a> for the example below. Loading might take a bit longer.</p> <div id="pyexample" style="border: 2px solid #ccc !important; border-radius: 5px; padding: 5px 5px 5px 5px;"> <py-script src="/assets/python_scripts/2022-05-21-gecco/simple_plot.py"></py-script> <canvas id="envcanvas" height="20" style="background-color: black; alignment: center; width: 100%"> Loading... </canvas> <div class="output-group"> <span class="output-group-btn"> <div style="display: inline-block">LeadingOnes: </div><div id="plot" style="display: inline-block; margin-right: 10px">0</div> <div style="display: inline-block">Steps Taken: </div><div id="timediv" style="display: inline-block">0</div> </span> </div> <div class="input-group"> <span class="input-group-btn"> <button type="button" class="btn btn-default" id="start-btn" style="alignment: left">START</button> <button type="button" class="btn btn-default" id="reset-btn" style="alignment: center">RESET</button> <button type="button" class="btn btn-default" id="stop-btn" style="alignment: right">STOP</button> <input type="range" min="1" max="100" value="1" step="1" class="slider" id="SpeedSlider" style=" -webkit-appearance: none; width: 21.5%; height: 10px; background: #d3d3d3; outline: none; opacity: 0.7; -webkit-transition: .2s; transition: opacity .2s; display: inline-block; margin-right: 10px; " oninput="this.nextElementSibling.value = this.value"/> Speed: <output>1</output>% <input type="range" min="1" max="15" value="1" step="1" class="slider" id="LambdaSlider" style=" -webkit-appearance: none; width: 73%; height: 10px; background: #d3d3d3; outline: none; opacity: 0.7; -webkit-transition: .2s; transition: opacity .2s; display: inline-block; margin-right: 10px; " oninput="this.nextElementSibling.value = this.value"/> Number of bitflips: <output>1</output> </span> </div> </div> <div class="figcaption"> <p> ${\color{gray}(1+1)}$RLS on a randomly initialized LeadingOnes problem. You can manually configure how many bits get flipped in each iteration via the lower slider. We only render the current best solution, thus some steps might not change the image. Cells that will not change anymore are colored in green. For pseudocode see <a href="#RLSpseudo">Algorithm 1</a>. </p> <br/> </div> <p>To achieve peak-performance on a problem, it is often crucial to correctly setup, i.e. configure, the algorithm which is supposed to solve the problem. In many communities it has been observed that fixed parameter choices are often not optimal and that dynamic changes to parameter during the algorithms run can be highly beneficial (see, e.g., <d-cite key="SA83,HansenO01,battiti-book08,moulines-neurips11,BurkeGHKOOQ13,daniel-aaai16,loshchilov-iclr17a, jaderberg-arxiv17a,DoerrD18ga,parker-holder-neurips20"></d-cite>). The evolutionary algorithms community is no stranger to this observation. Under the heading of <em>parameter control</em> (for an overview see <d-cite key="Doerr2020"></d-cite>) various methods have been proposed to adapt parameters on the fly. More importantly however, the community has provided various theoretical insights.</p> <p>In a similar vain to parameter control, the novel <em>dynamic algorithm configuration (DAC)</em> <d-cite key="biedenkapp-ecai20"></d-cite> framework proposes to learn parameter adaptation policies in a dedicated offline learning phase. Once a policy was learned, it can then be used to adapt algorithm parameters on a variety of problem instances. Still, the field is very young and there is much to learn. In our recently accepted GECCO paper <em><a href="https://andrebiedenkapp.github.io/assets/pdf/22-gecco.pdf"><font color="lightblue">Theory-Inspired Parameter Control Benchmarks for Dynamic Algorithm Configuration</font></a></em> builds on such insights of the parameter control community to build benchmarks which can be used to evaluate DAC methods and policies.</p> <h2 id="why-rls-on-leadingones">Why RLS on LeadingOnes?</h2> <p>The short answer is, LeadingOnes is very well understood. The slightly longer answer is, LeadingOnes is one of the most rigorously studied problems in the parameter control community. The parameter control community has thoroughly studied the dynamic fitness-dependent selection of mutation rates for greedy evolutionary algorithms on LeadingOnes. Thus, it is very well understood how the expected runtime of such algorithms depend on the mutation rates during the run. Overall LeadingOnes is an important benchmark for parameter control studies both for empirical <d-cite key="DoerrDY16PPSN,DoerrW18"></d-cite> and theoretical analysis <d-cite key="LissovoiOW20,DoerrLOW18LO,DoerrDL21"></d-cite>. Thus, this makes LeadingOnes an ideal benchmark to also study DAC methods in depth. Further, the theoretical insights can be used to provide a ground truth on LeadingOnes. This was not possible for prior DAC benchmarks <d-cite key="eimer-ijcai21"></d-cite>, besides some manually designed artificial benchmarks.</p> <h3 id="a-short-primer-on-leadingones-and-rls">A Short Primer on LeadingOnes and RLS</h3> <p>If you are already familiar with the LeadingOnes problem you can skip this short introduction. LeadingOnes is perfectly named. For a bitstring of length $\color{gray}n$, the LeadingOnes problem is to maximize the number of uninterrupted leading ones in the bitsring.</p> <p>There are a variety of algorithms one could use for solving LeadingOnes. Here, we chose to use $\color{gray}(1+1)$RLS. The pseudo code for this is:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">RLS</span><span class="p">(</span><span class="n">problem_dimension</span><span class="p">,</span> <span class="n">max_budget</span><span class="p">):</span>
    <span class="n">partial_sol</span> <span class="o">=</span> <span class="n">numpy</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">problem_dimension</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_budget</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="nf">get_current_state</span><span class="p">()</span>
        <span class="n">num_bits_to_flip</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">new_partial_sol</span> <span class="o">=</span> <span class="nf">flip</span><span class="p">(</span><span class="n">partial_sol</span><span class="p">,</span> <span class="n">num_bits_to_flip</span><span class="p">)</span>
        <span class="k">if</span> <span class="nf">fitness</span><span class="p">(</span><span class="n">new_partial_sol</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nf">fitness</span><span class="p">(</span><span class="n">partial_sol</span><span class="p">):</span>
            <span class="n">partial_sol</span> <span class="o">=</span> <span class="n">new_partial_sol</span></code></pre></figure> <div class="figcaption"> <p>Algorithm 1: Pseudocode for ${\color{gray}(1+1)}$RLS </p> </div> <p>The algorithm starts from a randomly initialized bitstring and in each iteration randomly flips <d-code language="python">num_bits_to_flip</d-code>. The so created new solution candidate is compared to the old one. The old one is replaced by the new one if the latter one is not worse than the former one. When using an algorithm for solving the LeadingOnes problem, we are interested in setting the algorithm up such that we solve the problem using as few function evaluations as possible, or in other words in as few iterations as possible. At the top of this post you can find a working implementation of this setup where you are in charge of setting the number of bits to flip in each iteration. If you’ve played around with this setting a bit, you might have noticed a few things:</p> <ul> <li>A too high number of bits to flip becomes detrimental the more leading ones we have.</li> <li>Always only flipping one bit is a valid strategy but might take a long time (depending on the initialization).</li> <li>Decreasing the number of bits to flip over time fairly reliably reduces the required running time.</li> </ul> <h3 id="ground-truth-and-optimal-policies">Ground Truth and Optimal Policies</h3> <p>It was proven <d-cite key="doerr-tcs19a"></d-cite> that we can compute the probability of improving the current partial solution of length $\color{gray}n$ with fitness value $\color{gray}i\leq n$ by flipping $\color{gray}r$ bits as \(\color{gray} q(r,i)=\frac{r}{n}\cdot\prod_{j\in\left[1,\ldots,r-1\right]}\frac{n-i-j}{n-j}\). Further, an optimal policy that can choose to flip any number of bits in $\color{gray}\left[1,\ldots,n\right]$ satisfies $\color{gray}\pi_{\text{opt}}\colon i\mapsto\lfloor n/(i+1)\rfloor$. Thus, if our current solution has fitness $\color{gray}0$ (i.e., no leading ones) we should flip all bits. If we only have one leading one, we should flip exactly $\color{gray}\lfloor n/2\rfloor$ bits, and so on. Let’s compare this optimal policy to static ones:</p> <div id="pyexample-comparison" style="border: 2px solid #ccc !important; border-radius: 5px; padding: 5px 5px 5px 5px;"> <py-script src="/assets/python_scripts/2022-05-21-gecco/comparison_leading_ones_full.py"></py-script> <div style="font-style: oblique; font-weight: bold; display: inline-block">Optimal Policy:</div> <div style="display: inline-block">$\color{gray}\pi_{\text{opt}}($<div style="display: inline-block" id="opt-f-val">?</div>$\color{gray})\mapsto$ </div><div id="r-comparison-optimal" style="display: inline-block">?</div> <canvas id="comparison-canvas-optimal" height="20" style="background-color: black; alignment: center; width: 100%"> Loading... </canvas> <div class="output-group-comparison-static"> <span class="output-group-btn-comparison-static"> <div style="display: inline-block">LeadingOnes: </div><div id="plot-comparison-optimal" style="display: inline-block; margin-right: 10px">0</div> <div style="display: inline-block">Steps Taken: </div><div id="timdiv-comparison-optimal" style="display: inline-block; margin-right: 10px">0</div> <div style="display: inline-block">#Solved: </div><div id="solved-comparison-optimal" style="display: inline-block; margin-right: 10px">0</div> <div style="display: inline-block">$\color{gray}\mu_{\text{steps}}$: </div><div id="avg-comparison-optimal" style="display: inline-block; margin-right: 10px">0</div> </span> </div> <div style="font-style: oblique; font-weight: bold">Static Policy:</div> <canvas id="comparison-canvas-static" height="20" style="background-color: black; alignment: center; width: 100%"> Loading... </canvas> <div class="output-group-comparison-static"> <span class="output-group-btn-comparison-static"> <div style="display: inline-block">LeadingOnes: </div><div id="plot-comparison-static" style="display: inline-block; margin-right: 10px">0</div> <div style="display: inline-block">Steps Taken: </div><div id="timediv-comparison-static" style="display: inline-block; margin-right: 10px">0</div> <div style="display: inline-block">#Solved: </div><div id="solved-comparison-static" style="display: inline-block; margin-right: 10px">0</div> <div style="display: inline-block">$\color{gray}\mu_{\text{steps}}$: </div><div id="avg-comparison-static" style="display: inline-block">0</div> </span> </div> <div class="input-group-comparison-static"> <span class="input-group-btn"> <button type="button" class="btn btn-default" id="start-btn-comparison-static" style="alignment: left">START</button> <button type="button" class="btn btn-default" id="reset-btn-comparison-static" style="alignment: center">RESET</button> <button type="button" class="btn btn-default" id="stop-btn-comparison-static" style="alignment: right">STOP</button> <input type="range" min="1" max="100" value="1" step="1" class="slider" id="SpeedSlider-comparison-static" style=" -webkit-appearance: none; width: 21.5%; height: 10px; background: #d3d3d3; outline: none; opacity: 0.7; -webkit-transition: .2s; transition: opacity .2s; display: inline-block; margin-right: 10px; " oninput="this.nextElementSibling.value = this.value"/> Speed: <output>1</output>% <input type="range" min="1" max="15" value="1" step="1" class="slider" id="LambdaSlider-comparison-static" style=" -webkit-appearance: none; width: 73%; height: 10px; background: #d3d3d3; outline: none; opacity: 0.7; -webkit-transition: .2s; transition: opacity .2s; display: inline-block; margin-right: 10px; " oninput="this.nextElementSibling.value = this.value"/> Number of bitflips: <output>1</output> </span> </div> </div> <div class="figcaption"> <p> </p> </div> <p>We can see that the optimal policy is quite a bit faster at solving the LeadingOnes example. However, what about policies that are restricted and cannot choose from all values in $\color{gray}\left[1,\ldots,n\right]$? In our paper, we present a method to compute the optimal policy for any such restricted choice. In essence, we can use the probability of improvement as defined above and only need to compare the probability of consecutive elements<d-footnote>We assume the portfolio is always sorted.</d-footnote> of the portfolio. Whenever we have a higher probability of improvement with the smaller portfolio element, we switch to that. So, for any $\color{gray}n$ and portfolio $\color{gray}\mathcal{K}$ we can compute the optimal policy and thus generate ground-truth about the behaviour of $\color{gray}(1+1)$RLS.<d-footnote>Note that 1 always needs to be included as we otherwise can't guarantee that a solution will be found.</d-footnote> This ability to compute the optimal policy for any portfolio (i.e., <em>configuration space</em>) makes this setting ideal to study how different problem sizes and configuration spaces influence DAC methods.</p> <h2 id="learning-dac-policies">Learning DAC Policies</h2> <p>Now that we know all about the benchmark, lets use it to gain insights into a DAC method<d-footnote>If you need a lightweight introduction to DAC we refer to <d-cite key="biedenkapp-automl20"></d-cite></d-footnote>. To give an example of the use-case of this benchmark we train a small DDQN agent to dynamically configure the number of bit flips of RLS on LeadingOnes based on the observed fitness value.</p> <p><img src="/assets/img/blog/2022-05-21-gecco/n50_policy.png" class="img-fluid rounded z-depth-1" data-zoomable="true" style="background-color: white; width: 49%; display: inline-block"/> <img src="/assets/img/blog/2022-05-21-gecco/n50_policies_k5.png" class="img-fluid rounded z-depth-1" data-zoomable="true" style="background-color: white; width: 49%; display: inline-block"/></p> <div class="figcaption"> <p> Comparison of a learned policy (dqn) compared to optimal ones. The dotted green line used the same restricted configuration space as the DQN. (left): The configuration space consists of three evenly spread values [1, 16, 32]. (right): The configuration space consists of five values [1, 2, 4, 8 16]. </p> </div> <p>Our first results (shown in the figures above) show that the DQN is indeed capable of learning optimal policies. Indeed on the same restricted configuration space for bitstrings of length 50, the DQN quickly learns the to play the optimal policy or ones that result in virtually the same reward.</p> <p>The availability of ground truth however not only enables us to compare the learned performance to the known optimal one, we can also study the limits of the chosen DAC method. For example, we can evaluate how the same DQNs learning capabilities scale with the size of the configuration space.</p> <p><img src="/assets/img/blog/2022-05-21-gecco/eval-n100_evenly_spread_boxplot.png" class="img-fluid rounded z-depth-1" data-zoomable="true" style="background-color: white"/></p> <div class="figcaption"> <p> Comparison of a learned policy (dqn) compared to optimal and random ones on problems of size 100 with varying portfolio sizes. For $\color{gray}k\geq7$ we plot three distinct runs to highlight the increased instability of DQN. For $\color{gray}k = 15$ none of the dqn runs were capable of learning meaningful policies. </p> </div> <p>Overall we could observe that the chosen DAC method, with its used parameter settings was capable of learning optimal policies but struggled to scale to larger action spaces as well as portfolio sizes. For more details we refer the interested reader to our paper.</p> <h2 id="conclusion">Conclusion</h2> <p>Our work presents a novel benchmark that is useful for both parameter control and DAC research. The benchmark fills an important gap in the existing benchmarks for DAC research as it enables us to compute optimal policies. We thus can easily evaluate the quality of learned DAC policies and as well as DAC techniques themselves. We showed how we can use the benchmark to gain insights into a DAC method and explored settings in which the chosen DQN method started to break down. We hope that our work is the first of many exchanges of benchmarks between the parameter control and dynamic algorithm configuration communities. With the growing literature on parameter control and its theoretical analysis we hope to provide other use-cases with a known ground truth.</p> <p>Our code and data are publicly available at <span style="color:blue"><a href="https://github.com/ndangtt/LeadingOnesDAC"><font color="lightblue">https://github.com/ndangtt/LeadingOnesDAC</font></a></span> and the benchmark has recently been merged into DACBench. For feedback or questions, feel free to reach out to us.</p>]]></content><author><name>André Biedenkapp</name></author><summary type="html"><![CDATA[Accompanying blog post for our GECCO'22 paper]]></summary></entry><entry><title type="html">The Importance of Hyperparameter Optimization for Model-based Reinforcement Learning</title><link href="https://andrebiedenkapp.github.io/blog/2021/redirect/" rel="alternate" type="text/html" title="The Importance of Hyperparameter Optimization for Model-based Reinforcement Learning"/><published>2021-04-19T12:00:00+00:00</published><updated>2021-04-19T12:00:00+00:00</updated><id>https://andrebiedenkapp.github.io/blog/2021/redirect</id><content type="html" xml:base="https://andrebiedenkapp.github.io/blog/2021/redirect/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[In depth discussion on importance of HPO for MBRL. Redicrects to the post on Berkeley AI Research.]]></summary></entry><entry><title type="html">AutoRL: AutoML for RL</title><link href="https://andrebiedenkapp.github.io/blog/2021/autorl-automl-for-rl/" rel="alternate" type="text/html" title="AutoRL: AutoML for RL"/><published>2021-04-19T10:47:36+00:00</published><updated>2021-04-19T10:47:36+00:00</updated><id>https://andrebiedenkapp.github.io/blog/2021/autorl-automl-for-rl</id><content type="html" xml:base="https://andrebiedenkapp.github.io/blog/2021/autorl-automl-for-rl/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Reinforcement learning (RL) has shown impressive results in a variety of applications. Well known examples include game and video game playing, robotics and, recently, “Autonomous navigation of stratospheric balloons”. A lot of the successes came about by combining the expressiveness of deep learning with the power of RL. Already on their own though, both frameworks [&#8230;]]]></summary></entry><entry><title type="html">Learning Step-Size Adaptation in CMA-ES</title><link href="https://andrebiedenkapp.github.io/blog/2020/learning-step-size-adaptation-in-cma-es/" rel="alternate" type="text/html" title="Learning Step-Size Adaptation in CMA-ES"/><published>2020-08-05T10:00:39+00:00</published><updated>2020-08-05T10:00:39+00:00</updated><id>https://andrebiedenkapp.github.io/blog/2020/learning-step-size-adaptation-in-cma-es</id><content type="html" xml:base="https://andrebiedenkapp.github.io/blog/2020/learning-step-size-adaptation-in-cma-es/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[In a Nutshell In CMA-ES, the step size controls how fast or slow a population traverses through a search space. Large steps allow you to quickly skip over uninteresting areas (exploration), whereas small steps allow a more focused traversal of interesting areas (exploitation). Handcrafted heuristics usually trade off small and large steps given some measure [&#8230;]]]></summary></entry><entry><title type="html">Dynamic Algorithm Configuration</title><link href="https://andrebiedenkapp.github.io/blog/2020/dynamic-algorithm-configuration/" rel="alternate" type="text/html" title="Dynamic Algorithm Configuration"/><published>2020-02-10T16:53:50+00:00</published><updated>2020-02-10T16:53:50+00:00</updated><id>https://andrebiedenkapp.github.io/blog/2020/dynamic-algorithm-configuration</id><content type="html" xml:base="https://andrebiedenkapp.github.io/blog/2020/dynamic-algorithm-configuration/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[By &#160; Motivation When designing algorithms we want them to be as flexible as possible such that they can solve as many problems as possible. To solve a specific family of problems well, finding well-performing hyperparameter configurations requires us to either use extensive domain knowledge or resources. The second point is especially true if we [&#8230;]]]></summary></entry><entry><title type="html">BOHB: Robust and Efficient Hyperparameter Optimization at Scale</title><link href="https://andrebiedenkapp.github.io/blog/2018/bohb-robust-and-efficient-hyperparameter-optimization-at-scale/" rel="alternate" type="text/html" title="BOHB: Robust and Efficient Hyperparameter Optimization at Scale"/><published>2018-07-26T15:00:48+00:00</published><updated>2018-07-26T15:00:48+00:00</updated><id>https://andrebiedenkapp.github.io/blog/2018/bohb-robust-and-efficient-hyperparameter-optimization-at-scale</id><content type="html" xml:base="https://andrebiedenkapp.github.io/blog/2018/bohb-robust-and-efficient-hyperparameter-optimization-at-scale/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[By Machine learning has achieved many successes in a wide range of application areas, but more often than not, these strongly rely on choosing the correct values for many hyperparameters (see e.g. Snoek et al., 2012). For example, we all know of the awesome results deep learning can achieve, but when we set its learning [&#8230;]]]></summary></entry><entry><title type="html">We did it again: world champions in AutoML</title><link href="https://andrebiedenkapp.github.io/blog/2018/we-did-it-again-world-champions-in-automl/" rel="alternate" type="text/html" title="We did it again: world champions in AutoML"/><published>2018-07-13T12:00:27+00:00</published><updated>2018-07-13T12:00:27+00:00</updated><id>https://andrebiedenkapp.github.io/blog/2018/we-did-it-again-world-champions-in-automl</id><content type="html" xml:base="https://andrebiedenkapp.github.io/blog/2018/we-did-it-again-world-champions-in-automl/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[By Our ML Freiburg lab is the world champion in automatic machine learning (AutoML) again! After winning the first international AutoML challenge (2015-2016), we also just won the second international AutoML challenge (2017-2018). Our system PoSH-Auto-sklearn outperformed all other 41 participating AutoML systems. What is AutoML and the scope of the competition? We all know [&#8230;]]]></summary></entry></feed>